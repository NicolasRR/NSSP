{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui wx\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#####################\n",
    "# Import of utils.py functions\n",
    "#####################\n",
    "# Required to get utils.py and access its functions\n",
    "notebook_dir = os.path.abspath(\"\")\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "sys.path.append('.')\n",
    "from utils import loadFSL, FSLeyesServer, mkdir_no_exist, interactive_MCQ\n",
    "\n",
    "####################\n",
    "# DIPY_HOME should be set prior to import of dipy to make sure all downloads point to the right folder\n",
    "####################\n",
    "os.environ[\"DIPY_HOME\"] = \"/home/jovyan/Data\"\n",
    "\n",
    "\n",
    "#############################\n",
    "# Loading fsl and freesurfer within Neurodesk\n",
    "# You can find the list of available other modules by clicking on the \"Softwares\" tab on the left\n",
    "#############################\n",
    "import lmod\n",
    "await lmod.purge(force=True)\n",
    "await lmod.load('fsl/6.0.7.4')\n",
    "await lmod.load('freesurfer/7.4.1')\n",
    "await lmod.list()\n",
    "\n",
    "####################\n",
    "# Setup FSL path\n",
    "####################\n",
    "loadFSL()\n",
    "\n",
    "###################\n",
    "# Load all relevant libraries for the lab\n",
    "##################\n",
    "import fsl.wrappers\n",
    "from fsl.wrappers import fslmaths\n",
    "\n",
    "import mne_nirs\n",
    "import nilearn\n",
    "from nilearn.datasets import fetch_development_fmri\n",
    "\n",
    "import mne\n",
    "import mne_nirs\n",
    "import dipy\n",
    "from dipy.data import fetch_bundles_2_subjects, read_bundles_2_subjects\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as op\n",
    "import nibabel as nib\n",
    "import glob\n",
    "\n",
    "import ants\n",
    "\n",
    "import openneuro\n",
    "from mne.datasets import sample\n",
    "from mne_bids import BIDSPath, read_raw_bids, print_dir_tree, make_report\n",
    "\n",
    "\n",
    "# Useful imports to define the direct download function below\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# FSL function wrappers which we will call from python directly\n",
    "from fsl.wrappers import fast, bet\n",
    "from fsl.wrappers.misc import fslroi\n",
    "from fsl.wrappers import flirt\n",
    "\n",
    "# General purpose imports to handle paths, files etc\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Start FSLeyes (very neat tool to visualize MRI data of all sorts) within Python\n",
    "################\n",
    "fsleyesDisplay = FSLeyesServer()\n",
    "fsleyesDisplay.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def direct_file_download_open_neuro(file_list, file_types, dataset_id, dataset_version, save_dirs):\n",
    "    # https://openneuro.org/crn/datasets/ds004226/snapshots/1.0.0/files/sub-001:sub-001_scans.tsv\n",
    "    for i, n in enumerate(file_list):\n",
    "        subject = n.split('_')[0]\n",
    "        download_link = 'https://openneuro.org/crn/datasets/{}/snapshots/{}/files/{}:{}:{}'.format(dataset_id, dataset_version, subject, file_types[i],n)\n",
    "        print('Attempting download from ', download_link)\n",
    "        download_url(download_link, op.join(save_dirs[i], n))\n",
    "        print('Ok')\n",
    "        \n",
    "def get_json_from_file(fname):\n",
    "    f = open(fname)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = 'ds000171' # This is the dataset ID for the OpenNeuro dataset we are using.\n",
    "subject_id = 'control01'    # This is the subject ID that we are using.\n",
    "\n",
    "sample_path = \"/home/jovyan/Data/dataset\" \n",
    "mkdir_no_exist(sample_path) # Creates the directory if it does not already exist.\n",
    "\n",
    "bids_root = op.join(sample_path, dataset_id) # Defines the BIDS root directory based on the dataset ID.\n",
    "deriv_root = op.join(bids_root, 'derivatives') # Defines the path for the derivatives directory, following BIDS standards.\n",
    "preproc_root = op.join(bids_root, 'derivatives', 'preprocessed_data') # Defines the path for the preprocessed data.\n",
    "\n",
    "mkdir_no_exist(bids_root) # Creates the BIDS root directory if it doesn't exist.\n",
    "\n",
    "subject_dir = 'sub-{}'.format(subject_id) # Formats the subject ID following the BIDS convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# OpenNeuro download.\n",
    "###################\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, # OpenNeuro assigns a unique ID for each dataset.\n",
    "                \"--target-dir\", bids_root,  # Specifies where the data will be saved locally.\n",
    "                \"--include\", op.join(subject_dir, '*'), # Downloads all files within the subject_dir   \n",
    "               ], check=True) # Ensures that the process raises an error if the download fails.\n",
    "\n",
    "###################\n",
    "# Create folders relevant for preprocessing.\n",
    "# In BIDS, ANY changes or preprocessing must be stored in the derivatives folder to keep the original data unaltered.\n",
    "###################\n",
    "mkdir_no_exist(op.join(bids_root, 'derivatives')) # Ensures the derivatives folder exists within the BIDS root.\n",
    "\n",
    "preproc_root = op.join(bids_root, 'derivatives', 'preprocessed_data') # Defines the directory for storing preprocessed data.\n",
    "mkdir_no_exist(preproc_root) # Creates the preprocessed data directory if it doesnâ€™t exist.\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-{}'.format(subject_id))) # Creates a folder for the subject in the preprocessed data directory.\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-{}'.format(subject_id), 'anat')) # Creates the folder for anatomical data.\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-{}'.format(subject_id), 'func')) # Creates the folder for functional (fMRI) data.\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-{}'.format(subject_id), 'fmap')) # Creates the folder for field map data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the functional JSON file in the BIDS dataset.\n",
    "# 'bids_root' is the root directory for the BIDS dataset.\n",
    "# 'sub-control01' is the subject's folder name, following BIDS conventions.\n",
    "# 'func' is the folder where functional data (fMRI) is stored.\n",
    "# 'sub-control01_task-music_run-1_bold.json' is the specific JSON file containing metadata for the fMRI scan.\n",
    "json_path = op.join(bids_root, 'sub-control01', 'anat', 'sub-control01_task-music_run-1_bold.json') \n",
    "\n",
    "# Use the function 'get_json_from_file' to read the JSON file and return its contents as a dictionary.\n",
    "data = get_json_from_file(json_path)\n",
    "\n",
    "# Display the contents of the JSON file stored in 'data'\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(bids_root, 'sub-control01', 'anat', 'sub-control01_T1w.nii.gz')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skull stripping\n",
    "\n",
    "###  Preprocessing and BIDs\n",
    "An important part of **anatomical** preprocessing is to remove the skull around the brain.\n",
    "To adhere to the BIDs format, all modified files should be put in a new folder, called derivatives, such that you always have clean data in the source directory. The derivatives folder can be used for different preprocessing and treatments, each needing their own subfolders. In our case, we've created a single folder, preprocessed_data, hence the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skull_stripped_anatomical(bids_root, preproc_root, subject_id, robust=False): \n",
    "    \"\"\"\n",
    "    Function to perform skull-stripping (removing the skull around the brain).\n",
    "    This is a simple wrapper around the brain extraction tool (BET) in FSL's suite.\n",
    "    It assumes data to be in the BIDS format.\n",
    "\n",
    "    The method also saves the brain mask which was used to extract the brain.\n",
    "\n",
    "    The brain extraction is conducted only on the T1w of the participant.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bids_root: string\n",
    "        The root of the BIDS directory.\n",
    "    preproc_root: string\n",
    "        The root of the preprocessed data, where the result of the brain extraction will be saved.\n",
    "    subject_id: string\n",
    "        Subject ID, the subject on which brain extraction should be conducted.\n",
    "    robust: bool\n",
    "        Whether to conduct robust center estimation with BET or not. Default is False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Format the subject ID following the BIDS convention (e.g., 'sub-001')\n",
    "    subject_dir = 'sub-{}'.format(subject_id) \n",
    "    \n",
    "    # Define the path to the anatomical T1-weighted image for the subject\n",
    "    anatomical_path = op.join(bids_root, subject_dir, 'anat', 'sub-{}_T1w.nii.gz'.format(subject_id)) \n",
    "\n",
    "    # Define the path where the skull-stripped brain will be saved\n",
    "    betted_brain_path = op.join(preproc_root, subject_dir, 'anat', 'sub-{}_T1w'.format(subject_id)) \n",
    "\n",
    "    # Run FSL's brain extraction tool (BET) using the os.system command\n",
    "    # - If 'robust' is True, the '-R' option is used for robust center estimation\n",
    "    # - Otherwise, no additional flag is used\n",
    "    os.system('bet {} {} -m {}'.format(anatomical_path, betted_brain_path, '-R' if robust else ''))\n",
    "\n",
    "    # Print a message indicating the process is complete\n",
    "    print(\"Done with BET.\")\n",
    "\n",
    "#  This sets the path where the brain mask will be stored. Takes as input the file stored in betted_brain_path from BET\n",
    "resulting_mask_path = op.join(preproc_root, subject_dir, 'anat', 'sub-{}_T1w_mask'.format(subject_id) ) \n",
    "\n",
    "get_skull_stripped_anatomical(bids_root, preproc_root, \"control01\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(resulting_mask_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the fit\n",
    "If you look a bit into bet's documentation, you'll quickly find that there are parameters with which you can play; robust brain centre estimation and fractional intensity threshold. To demonstrate the importance and impact of these parameters, let's use a robust brain center estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_skull_stripped_anatomical(bids_root, preproc_root, \"control01\", robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset any existing overlays in FSLeyes to start with a clean display\n",
    "fsleyesDisplay.resetOverlays()\n",
    "\n",
    "# Load the T1-weighted anatomical image of subject 'sub-001' into FSLeyes\n",
    "# The path is constructed by joining the BIDS root with the subject directory, the 'anat' folder,\n",
    "# and the file name 'sub-001_T1w.nii.gz' which follows the BIDS convention.\n",
    "fsleyesDisplay.load(op.join(bids_root, 'sub-control01', 'anat', 'sub-control01_T1w.nii.gz'))\n",
    "\n",
    "# Load a mask overlay onto the anatomical image\n",
    "# The 'resulting_mask_path' variable should point to a mask file that highlights specific regions\n",
    "# or structures of interest within the anatomical image.\n",
    "fsleyesDisplay.load(resulting_mask_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Manual corrections\n",
    "If you really want good fit, you might want to resort to **manually correcting the mask**. Run the code above to check whether the result is satisfactory or not. \n",
    "\n",
    "FSLeyes readily allows you to do such things! While on FSLeyes, press **Alt + E** to open the editing interface.\n",
    "\n",
    ">Edit\n",
    "\n",
    "\n",
    "You can then escape back to the non-edit mode by pressing again **Alt + E**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Fill me with the code to use your mask\n",
    "# To help you, we provide you with the skeleton of two potential approaches.\n",
    "# You can fill either of them. Do not forget to test them by visualizing the result!\n",
    "# For fslmaths, you can either read the documentation, or execute it without argument by running os.system('fslmaths')\n",
    "##################################\n",
    "\n",
    "\n",
    "# Based on the code and the explanations I provided earlier, the value to insert in `mask_path` should be the path to the mask file generated during the skull-stripping process using BET. \n",
    "# This file is indicated by `resulting_mask_path`.\n",
    "\n",
    "# Option 1: Pythonic approach.\n",
    "def apply_python_mask_approach(img_path, mask_path, masked_img_path):\n",
    "    \"\"\"\n",
    "    The first approach, Pythonic way. The goal is, given a mask, to apply it to a T1 image where the brain is to be extracted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path: str\n",
    "        Path to the image on which we would like to apply the mask (in this case, the T1 with the skull still on). Should be a .nii.gz file.\n",
    "    mask_path: str\n",
    "        Path to the mask you would like to apply to your image. Should be a .nii.gz file, containing only binary values (0 or 1).\n",
    "    masked_img_path: str\n",
    "        Path to which the resulting masked image will be saved. \n",
    "    \"\"\"\n",
    "    import nibabel as nib\n",
    "\n",
    "    # Load both the T1 image and the mask from disk\n",
    "    img = nib.load(img_path)\n",
    "    mask = nib.load(mask_path)\n",
    "    \n",
    "    # Extract the image data and mask data as numpy arrays. \n",
    "    img_data = img.get_fdata()\n",
    "    mask_data = mask.get_fdata()\n",
    "\n",
    "    #######################\n",
    "    # Solution 1\n",
    "    # Create an empty image array and fill it with values where the mask is greater than 0\n",
    "    #######################\n",
    "    # Initialize an empty array with the same shape as the T1 image\n",
    "    saved_img_data = np.zeros(img_data.shape)\n",
    "    # Fill the array with image data only where the mask is greater than 0 (brain region)\n",
    "    saved_img_data[mask_data > 0] = img_data[mask_data > 0]\n",
    "\n",
    "    # Save the masked image to disk by creating a new Nifti image and writing it out\n",
    "    img_out = nib.Nifti1Image(saved_img_data, img.affine, img.header)\n",
    "    nib.save(img_out, masked_img_path)\n",
    "\n",
    "    #######################\n",
    "    # Solution 2\n",
    "    # An alternative approach: set all values outside the mask (where mask = 0) to zero.\n",
    "    #######################\n",
    "    \n",
    "    # Set all image data values to 0 where the mask is equal to 0 (outside the brain region)\n",
    "    img_data[mask_data == 0] = 0\n",
    "    \n",
    "    # Save the modified image to disk, creating a new Nifti image and writing it out\n",
    "    img_out = nib.Nifti1Image(img_data, img.affine, img.header)\n",
    "    nib.save(img_out, masked_img_path)\n",
    "    \n",
    "def apply_fsl_math_approach(img_path, mask_path, masked_img_path):\n",
    "    ###########################\n",
    "    # Solution\n",
    "    # Based on fslmaths documentation, the -mas option is used to apply a mask to the image.\n",
    "    ###########################\n",
    "    os.system('fslmaths {} -mas {} {}'.format(img_path, mask_path, masked_img_path))\n",
    "    \n",
    "\n",
    "# Define the path to the original T1 image of the brain\n",
    "anatomical_path = op.join(bids_root, 'sub-control01', 'anat', 'sub-control01_T1w.nii.gz') # The original brain\n",
    "\n",
    "# Define the path where the skull-stripped brain will be saved (in the derivatives folder)\n",
    "betted_brain_path = op.join(preproc_root, 'sub-control01', 'anat', 'sub-control01_T1w.nii.gz') \n",
    "\n",
    "# Define the path to the brain mask generated from the skull-stripping process\n",
    "resulting_mask_path = op.join(preproc_root, 'sub-control01', 'anat', 'sub-control01_T1w_mask.nii.gz') \n",
    "\n",
    "########################\n",
    "# CHOOSE ONE OF THE TWO TO IMPLEMENT IT AND LAUNCH IT\n",
    "########################\n",
    "# Apply the mask using FSL's fslmaths tool. This approach applies the mask directly using command-line tools.\n",
    "# apply_fsl_math_approach(anatomical_path, resulting_mask_path, betted_brain_path)\n",
    "\n",
    "# Apply the mask using the Python-based approach. This reads the images into Python, applies the mask, and saves the result.\n",
    "apply_python_mask_approach(anatomical_path, resulting_mask_path, betted_brain_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, do not forget to visualize what you have done. If you did a proper job, you should now see the brain without any skull!\n",
    "\n",
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(betted_brain_path)\n",
    "\n",
    "# Note: If you get a RuntimeError: wrapped C/C++ object of type OrthoEditActionToolBar has been deleted, do not worry, \n",
    "# this is simply because you forgot to escape back to non-edit mode (Alt+E) before resetting the overlay, but it is a benign error :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tissue segmentation\n",
    "\n",
    "For the purpose of analysis, it can be useful to separate the tissues into tissue classes; in particular extracting the white matter, grey matter and cerebrospinal fluid (abreviated as CSF) is very interesting in fMRI analysis\n",
    "\n",
    "It is perfectly possible (even likely) that FSLeyes will stop responding over the course of this lab. This is perfectly normal! Simply wait for whichever function (such as FAST) to finish and it should start responding again, don't worry too quickly, be patient :)</p>\n",
    "\n",
    "\n",
    "Note that FAST will take one or two minutes to run, this is expected, do not panic :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the original T1-weighted anatomical image for subject 'sub-001'.\n",
    "# This is the raw, unprocessed anatomical scan with the skull.\n",
    "anatomical_path = op.join(bids_root, 'sub-control01', 'anat', 'sub-control01_T1w.nii.gz')\n",
    "\n",
    "# Define the path to the skull-stripped image (brain-extracted) for subject 'sub-001'.\n",
    "# This path points to the image generated after applying BET, stored in the derivatives folder.\n",
    "\n",
    "# Recall: betted_brain_path = op.join(preproc_root, 'sub-001', 'anat', 'sub-001_T1w.nii.gz') \n",
    "\n",
    "bet_path = op.join(preproc_root, 'sub-control01', 'anat', 'sub-control01_T1w')\n",
    "\n",
    "#########################\n",
    "# Solution\n",
    "# We need to apply FAST (FSLâ€™s Automated Segmentation Tool) to the brain-extracted image.\n",
    "# Therefore, we use the BET path as the target since it has the skull removed.\n",
    "##########################\n",
    "\n",
    "fast_target = bet_path # Assign the skull-stripped image path (bet_path) to fast_target.\n",
    "# Note: You could experiment with using anatomical_path instead, but the brain-extracted image (bet_path) is more appropriate.\n",
    "\n",
    "# Remove any existing FAST segmentation files in the directory to ensure a clean environment for processing.\n",
    "# This cleans up any files that have \"fast\" in their names in the subject's anat folder.\n",
    "[os.remove(f) for f in glob.glob(op.join(preproc_root, 'sub-control01', 'anat', '*fast*'))]\n",
    "\n",
    "# Define the path where the output of FAST segmentation will be stored.\n",
    "# The output will have a prefix 'sub-001_T1w_fast' indicating that it is the FAST segmentation result.\n",
    "segmentation_path = op.join(preproc_root, 'sub-control01', 'anat', 'sub-control01_T1w_fast')\n",
    "\n",
    "# Apply\n",
    "fast(imgs=[fast_target], out=segmentation_path, n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the quality of the segmentation, shall we?\n",
    "We want to extract 3 tissue types here: the white matter, the grey matter and the csf. How well did fast perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pve files correspond to our segmented tissues. We have exactly three files, because we set n_classes to 3 above:\n",
    "```python\n",
    "fast(..., n_classes=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier on you, we will display:\n",
    "\n",
    "- pve_0 in <span style=\"color:red;\">red</span>\n",
    "- pve_1 in <span style=\"color:green;\">green</span>\n",
    "- pve_2 in <span style=\"color:blue;\">blue</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset any existing overlays in FSLeyes to start with a clean visualization\n",
    "fsleyesDisplay.resetOverlays()\n",
    "\n",
    "# Load the skull-stripped T1-weighted image (brain-only) as the base image.\n",
    "fsleyesDisplay.load(bet_path)\n",
    "\n",
    "# Load the first partial volume estimate (PVE) map, which typically represents cerebrospinal fluid (CSF).\n",
    "# The file is identified using a glob pattern that matches files containing 'pve_0' in their name.\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_root, 'sub-control01', 'anat','*pve_0*'))[0])\n",
    "\n",
    "# Load the second PVE map, which typically represents gray matter (GM).\n",
    "# The file is identified using a glob pattern that matches files containing 'pve_1' in their name.\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_root, 'sub-control01', 'anat','*pve_1*'))[0])\n",
    "\n",
    "# Load the third PVE map, which typically represents white matter (WM).\n",
    "# The file is identified using a glob pattern that matches files containing 'pve_2' in their name.\n",
    "fsleyesDisplay.load(glob.glob(op.join(preproc_root, 'sub-control01', 'anat','*pve_2*'))[0])\n",
    "\n",
    "# Set the colormap for the first PVE overlay (CSF) to 'Red'.\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[1]).cmap = 'Red'\n",
    "\n",
    "# Set the colormap for the second PVE overlay (GM) to 'Green'.\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[2]).cmap = 'Green'\n",
    "\n",
    "# Set the colormap for the third PVE overlay (WM) to 'Blue'.\n",
    "fsleyesDisplay.displayCtx.getOpts(fsleyesDisplay.overlayList[3]).cmap = 'Blue'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coregistration of images, a critical preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, it could be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def launch_freeview(img_list):\n",
    "    \"\"\"\n",
    "    Wrapper around Freeview to launch it with several images.\n",
    "    This wrapper is necessary to launch Freeview in a separate thread, ensuring the notebook is free to do something else.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_list: list of string\n",
    "        List of images (files) to load. Assumed by default to be volume files.\n",
    "    \"\"\"\n",
    "    args = []\n",
    "    \n",
    "    # Iterate over the list of images and prepare the command arguments for Freeview\n",
    "    for i in range(len(img_list)):\n",
    "        args.append(\"-v\")        # '-v' is used in Freeview to specify the volume files\n",
    "        args.append(img_list[i]) # Append each image file path to the arguments list\n",
    "\n",
    "    # Run the Freeview command with all the prepared arguments\n",
    "    subprocess.run([\"freeview\"] + args)\n",
    "\n",
    "# List of images to load in Freeview. \n",
    "# img_list contains paths to neuroimaging files you want to visualize:\n",
    "# A standard anatomical template (MNI152) for reference.\n",
    "# A subject-specific anatomical image with a specific colormap (greyscale).\n",
    "imgList = [\n",
    "    op.expandvars('$FSLDIR/data/standard/MNI152_T1_1mm.nii.gz'), # Path to the standard MNI template image\n",
    "    op.join(preproc_root, \"sub-control01\", \"anat\", \"sub-control01_T1w.nii.gz:colormap=greyscale\") # Path to the subject's anatomical image with colormap set to greyscale\n",
    "    # You can modify this list to add any other images you want to view in .nii.gz format\n",
    "]\n",
    "\n",
    "# Create a new thread to run Freeview with the list of images\n",
    "# We pass the target function (launch_freeview) and the arguments (imgList,)\n",
    "# The comma is necessary to pass a tuple to 'args' so threading works correctly\n",
    "freeview_thread = threading.Thread(target=launch_freeview, args=(imgList,))\n",
    "\n",
    "# Start the thread to launch Freeview in the background\n",
    "freeview_thread.start()\n",
    "\n",
    "# Print a message indicating that Freeview is running in a separate thread\n",
    "print(\"Freeview is running in a separate thread.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, we can't see anything apart from the top brain. No worries! Simply set the background to be transparent by ticking the Clear Background option, as in the picture below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a straightforward approach: you will align the images manually. In Freeview, click in Tools > Transform Volume.\n",
    "You should get the following panel:\n",
    "\n",
    "Now, play with the sliders of translation and rotation to align the anatomical to the reference. </b> Try to align the two brains as best you can.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MRI and the template are not very well aligned, but we can try to make them more aligned. Specifically, we would like to find a transformation such that we can align our anatomical to the MNI template. This is the so-called normalization step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of normalization\n",
    "\n",
    "So, you now know that you need a transformation and a reference. Great. Now, the transformation you allow can be of two types: it can be linear, meaning whatever you apply will be the same across the entire image, or non linear, where each voxel gets a separate treatment\n",
    "\n",
    "(ducky linear and non linear)\n",
    "\n",
    "\n",
    "\n",
    "## Actually doing it: Linear normalization\n",
    "\n",
    "To perform linear normalization, the idea is simple. The transformation we want should be linear - ie, affine.\n",
    "Such a matching is usually called in image processing **image registration**. Here, we're dealing with 3D data, so the problem is a bit more complicated. Fortunately all of this has been coded by very smart people, and to our rescue comes a tool specifically to register volumes to each other.\n",
    "\n",
    "This tool can allow many registrations and is extremely powerful. In its most basic form, it expects:\n",
    "- An input volume, the volume you want to register (Ducky's sunglasses)\n",
    "- A reference volume, to which the input is registered (Ducky's body)\n",
    "- An output volume, the result of the transformation (Ducky's sunglasses once they are on Ducky's beak)\n",
    "\n",
    "Here is how you can call it to register the patient's anatomical to some reference sitting in another space (here the MNI152 template):\n",
    "```python\n",
    "flirt()\n",
    "```\n",
    "ðŸ’¡ Pay attention! ðŸ’¡\n",
    "    FLIRT expects the anatomical to be skull stripped to maximize normalization. Luckily, you already did it before with BET.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsl.wrappers import flirt\n",
    "# Import the FLIRT (FMRIB's Linear Image Registration Tool) wrapper from the FSL library to perform image registration.\n",
    "\n",
    "# The two images\n",
    "subject_id = 'control01' \n",
    "# Define the subject ID. This will be used to specify the path to the subject's anatomical image.\n",
    "\n",
    "subject_anatomical = op.join(preproc_root, 'sub-{}'.format(subject_id), 'anat', 'sub-control01_T1w')\n",
    "# Define the path to the subject's T1-weighted anatomical image, located in the preprocessed data directory ('preproc_root') \n",
    "# under the subject's anatomical folder ('anat').\n",
    "\n",
    "mni_template = op.expandvars(op.join('$FSLDIR', 'data', 'standard', 'MNI152_T1_1mm_brain'))\n",
    "# Define the path to the MNI152 standard brain template, which is often used as a reference for alignment.\n",
    "# The path is constructed using the FSL environment variable '$FSLDIR' and expands it to the full directory path.\n",
    "\n",
    "###################\n",
    "# Select which image should be target or reference\n",
    "# ANSWER:\n",
    "# The subject anatomical will most often be the target, as the template is usually where we want to map our subjects for \n",
    "# group comparison.\n",
    "# There are cases, however, where we may want the subject anatomical to be the reference. This is the case when we want to map an \n",
    "# atlas to a subject while preserving the subject as much as possible.\n",
    "# We showcase here the case where the subject is chosen as target.\n",
    "##################\n",
    "\n",
    "target = subject_anatomical # Set the subject anatomical image as the target image to be aligned.\n",
    "reference = mni_template # Set the MNI template as the reference image (the space to which the target will be aligned).\n",
    "\n",
    "result = op.join(preproc_root, 'sub-{}'.format(subject_id), 'anat', 'sub-{}_T1w_mni'.format(subject_id))\n",
    "# Define the path where the output of the registration will be saved.\n",
    "# This path includes the subject ID and saves the aligned image in the anatomical folder under 'preproc_root'.\n",
    "# The output filename includes the suffix '_T1w_mni' to indicate that it is aligned to the MNI space.\n",
    "\n",
    "flirt(target, reference, out=result)\n",
    "# Call the FLIRT function to perform the registration:\n",
    "# - 'target': The image to be transformed (subject anatomical image).\n",
    "# - 'reference': The image to align to (MNI template).\n",
    "# - 'out': The path where the registered image will be saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the result of flirt on top of the reference. What do you think of alignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(reference) \n",
    "fsleyesDisplay.load(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which cost should we use? If you were in a pure void, there would be no right or wrong answer from the get-go. No choice but to experiment and find out!\n",
    "\n",
    "Hopefully, <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/flirt/user_guide?id=flirt\">the documentation</a> should give you some pointers. What you want here is to register a T1 to a T1: this is a <u>within</u> modality registration, so you should restrict yourself only to costs appropriate to this type of modality! \n",
    "\n",
    "To help you, we've set up a cell that will run the different coregistrations for you. Simply fill in the different costs to consider :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# Solution\n",
    "# We consider only within-modalities costs because the two images belong to the same modality (T1-weighted images).\n",
    "# Therefore, appropriate costs include least squares and normalized correlation ratio.\n",
    "# Note: Correlation ratio (corratio) can also be used, but it is more commonly applied for different modalities.\n",
    "#######\n",
    "possible_costs = ['mutualinfo', 'corratio', 'normcorr', 'normmi', 'leastsq', 'labeldiff']\n",
    "# Define a list of all possible cost functions supported by FLIRT. These include:\n",
    "# - 'mutualinfo': Mutual information (cross-modalities).\n",
    "# - 'corratio': Correlation ratio (can work across or within modalities).\n",
    "# - 'normcorr': Normalized correlation (within-modalities).\n",
    "# - 'normmi': Normalized mutual information (cross-modalities).\n",
    "# - 'leastsq': Least squares (within-modalities).\n",
    "# - 'labeldiff': Label difference (special cases, such as segmentation images).\n",
    "\n",
    "costs_to_consider = ['leastsq', 'normcorr'] \n",
    "# Select the relevant cost functions for within-modalities (T1-to-T1 alignment):\n",
    "# - 'leastsq': Uses least squares, suitable for aligning images of the same modality.\n",
    "# - 'normcorr': Uses normalized correlation, also suitable for within-modality alignment.\n",
    "\n",
    "# Iterate through the selected cost functions and apply FLIRT for each\n",
    "for c in costs_to_consider:\n",
    "    flirt(target, reference, out=result + '_' + c, cost=c)\n",
    "    # Run FLIRT using the specified cost function:\n",
    "    # - 'target': The image to be transformed (subject anatomical image).\n",
    "    # - 'reference': The image to align to (MNI template).\n",
    "    # - 'out': Save the output file with a suffix based on the cost function used (e.g., '_leastsq' or '_normcorr').\n",
    "    # - 'cost': The cost function to be used for optimization ('leastsq' or 'normcorr' in this loop).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in costs_to_consider:\n",
    "    fsleyesDisplay.load(result + '_' + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non linear normalization\n",
    "\n",
    "So, you know how to do it linearly.  What if we wanted to do it non-linearly?\n",
    "\n",
    "With FLIRT, <i>it's painfully hard</i>. To do it, you can use *FNIRT*. \n",
    "\n",
    "But, there are other tools available, one of them being <a href=\"https://github.com/ANTsX/ANTs\">ANTs (Advanced Normalization Tools)</a>.\n",
    "For completeness, we will show you now how to use it (very succinctly) so that you know how to do it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ’¡ Pay attention! ðŸ’¡\n",
    "\n",
    "FNIRT does NOT expect the input data to be skull-stripped.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the moving image (subject anatomical image) using the ANTs library.\n",
    "# The image path is constructed by adding '.nii.gz' to the target path.\n",
    "# target = subject_anatomical # Set the subject anatomical image as the target image to be aligned.\n",
    "moving_image = ants.image_read(target + '.nii.gz')\n",
    "\n",
    "# Read the fixed image (MNI template) using the ANTs library.\n",
    "# The image path is constructed by adding '.nii.gz' to the reference path.\n",
    "# reference = mni_template # Set the MNI template as the reference image (the space to which the target will be aligned).\n",
    "\n",
    "fixed_image = ants.image_read(reference + '.nii.gz')\n",
    "\n",
    "# Compute the transformation to align the moving image to the fixed image using a non-linear registration method.\n",
    "# 'SyN' (Symmetric Normalization) is a non-linear transformation method often used for precise alignment.\n",
    "# The function returns a dictionary containing the transformation details, including the forward transformation.\n",
    "transformation = ants.registration(fixed=fixed_image, moving=moving_image, type_of_transform='SyN')\n",
    "\n",
    "# Apply the computed transformation to the moving image to warp it into alignment with the fixed image.\n",
    "# 'fixed=fixed_image': The reference image that serves as the alignment target.\n",
    "# 'moving=moving_image': The image to be transformed.\n",
    "# 'transformlist=transformation['fwdtransforms']': Use the forward transformation obtained from the registration.\n",
    "warpedImage = ants.apply_transforms(fixed=fixed_image, moving=moving_image, transformlist=transformation['fwdtransforms'])\n",
    "\n",
    "# Define the output path for the transformed image.\n",
    "# The path includes the subject ID and indicates that the image has been aligned using the SyN method.\n",
    "resultAnts = op.join(preproc_root, 'sub-{}'.format(subject_id), 'anat', 'sub-{}_T1w_mni_SyN.nii.gz'.format(subject_id))\n",
    "\n",
    "# Save the warped image (aligned to the MNI template) to disk using the specified path.\n",
    "ants.image_write(warpedImage, resultAnts)\n",
    "\n",
    "# The resulting image can be inspected using FSLeyes or Freeview for visual verification of the alignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the results and compare it against the linear coregistration. Which one do you prefer? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(reference)\n",
    "fsleyesDisplay.load(result)\n",
    "fsleyesDisplay.load(resultAnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a final note, all these steps (<u>including</u> non linear normalization!) can be done automatically for you with a single command: <a href=\"https://web.mit.edu/fsl_v5.0.10/fsl/doc/wiki/fsl_anat.html\">fsl_anat</a>. So you might want to use this command, instead of running all of the above when conducting preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide it here for convenience, but beware: it takes <b>several minutes</b> to complete, so you will need some patience!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('fsl_anat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def fsl_anat_wrapped(anatomical_target, output_path):\n",
    "    # Run the 'fsl_anat' command to perform automatic brain extraction and segmentation on the anatomical image.\n",
    "    # - 'anatomical_target': Path to the anatomical image to process.\n",
    "    # - '--clobber': Overwrite any existing output with the same name.\n",
    "    # - '--nosubcortseg': Skip subcortical segmentation to speed up processing.\n",
    "    # - '-o': Specifies the output directory where results will be saved.\n",
    "    os.system('fsl_anat -i {} --clobber --nosubcortseg -o {}'.format(anatomical_target, output_path))\n",
    "    \n",
    "    # Define the path to the folder created by FSL ('output_path.anat').\n",
    "    # FSL saves the output files in this directory by default.\n",
    "    fsl_anat_path = output_path + '.anat'\n",
    "    \n",
    "    # Find all files in the 'output_path.anat' folder.\n",
    "    files_to_move = glob.glob(op.join(fsl_anat_path, '*'))\n",
    "    \n",
    "    # Move each file from the 'output_path.anat' folder to the main 'output_path' folder.\n",
    "    for f in files_to_move:\n",
    "        # 'shutil.move' moves each file to the output directory.\n",
    "        # 'op.split(f)[1]' gets the file name (e.g., 'T1_brain.nii.gz') from the full path.\n",
    "        shutil.move(f, op.join(output_path, op.split(f)[1]))\n",
    "    \n",
    "    # Remove the now-empty 'output_path.anat' directory, as all files have been moved.\n",
    "    os.rmdir(fsl_anat_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsl_anat_wrapped(anatomical_path, op.join(preproc_root, 'sub-001', 'anat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> ðŸ’¡ Careful ! ðŸ’¡</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Some of the fMRI preprocessing you will see below leverage the anatomical MRI being preprocessed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fMRI preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one assumption with fMRI preprocessing is that you've already conducted the anatomical preprocessing. In particular, the two main steps you will need today (repeat it from lab 2) before launching the lab:\n",
    "- T1 skull-stripping (use BET)\n",
    "- T1 segmentation (use FAST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problematic volumes removal\n",
    "\n",
    "To open one particular volume, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(bids_root, 'sub-control01', 'func', 'sub-control01_task-music_bold.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field stabilization\n",
    "\n",
    "The scanner's field takes some time to settle. You probably noticed that the initial volume had a high contrast that quickly decayed to some baseline? It is precisely caused by the scanner's field settling.\n",
    "\n",
    "These scans are called *non-stationary volumes*, because they are acquired while the B0 field is not yet not stable.\n",
    "\n",
    "There's little to be done in this regard; we can only throw away the volumes that are contaminated in this specific case, to ensure this change in global signal does not drive our analysis.\n",
    "\n",
    "If we're being really formal, a higher overall contrast can be detected by looking at the mean voxel value in each volume, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "\n",
    "plt.plot(nib.load(op.join(bids_root, 'sub-control01', 'func', 'sub-control01_task-music_bold.nii.gz')).get_fdata().mean(axis=(0,1,2)))\n",
    "plt.xlabel('Time (volume)')\n",
    "plt.ylabel('Mean voxel intensity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll **choose** to discard the first 10 volumes\n",
    "\n",
    "Most scanners nowadays actually acquire few scans to help the B0 field settle. These are called dummy scans; the scanner acquires them but throws them away, meaning that you end up with a result that is settled. You should pay attention however if you ever analyze older datasets, as they will not benefit from these new techs obviously!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This script considers a test file reported in a lab, feel free to modify it ! ###\n",
    "\n",
    "# file_to_trim = glob.glob(op.join(bids_root, 'sub-control01', 'func', 'sub-control01_task-music_bold.nii.gz'))[0]\n",
    "# Search for the functional MRI (fMRI) file matching the specified pattern using glob.\n",
    "# The file path is constructed to locate the BOLD image for subject 'sub-01' in the 'ses-test' session under the 'func' directory.\n",
    "# The '[0]' at the end retrieves the first match found.\n",
    "\n",
    "# mkdir_no_exist(preproc_root)\n",
    "# Create the root directory for preprocessed data if it doesn't already exist. This will be used to store the output files.\n",
    "\n",
    "# mkdir_no_exist(op.join(preproc_root, 'sub-control01'))\n",
    "# Create a directory for subject 'sub-control01' under the preprocessed data directory if it doesn't exist.\n",
    "\n",
    "# mkdir_no_exist(op.join(preproc_root, 'sub-control01', 'ses-test'))\n",
    "# Create a directory for the session 'ses-test' within the subject's directory under the preprocessed data directory.\n",
    "\n",
    "# mkdir_no_exist(op.join(preproc_root, 'sub-control01', 'ses-test','func'))\n",
    "# Create the 'func' directory within the session folder. This directory will hold the functional data after processing.\n",
    "\n",
    "# output_target = op.join(preproc_root, 'sub-control01', 'ses-test', 'func', 'sub-01_ses-test_task-fingerfootlips_bold_settled.nii.gz')\n",
    "# Define the output path for the trimmed fMRI data. \n",
    "# The path includes the subject ID, session ID, and specifies the name for the new file ('sub-01_ses-test_task-fingerfootlips_bold_settled.nii.gz').\n",
    "\n",
    "############################\n",
    "# Solution\n",
    "# We will start from the 10th volume.\n",
    "# Because indexing starts at 0, the 11th volume corresponds to index 10, so we exclude the first 10 volumes.\n",
    "# The original dataset has 184 volumes. By excluding the first 10, we retain 174 volumes.\n",
    "###########################\n",
    "\n",
    "# Define the variables for the starting volume and the number of volumes to keep\n",
    "# start_vol = 10 # The volume index to start from (0-based indexing, so this is the 11th volume).\n",
    "# number_of_volumes = 174 # The number of volumes to keep after trimming, leaving 174 volumes from the original 184.\n",
    "\n",
    "# fslroi(file_to_trim, output_target, str(start_vol), str(number_of_volumes))\n",
    "# Run the 'fslroi' command to extract a subset of volumes from the original fMRI file.\n",
    "# - 'file_to_trim': The path to the original fMRI file.\n",
    "# - 'output_target': The path where the trimmed file will be saved.\n",
    "# - 'start_vol': The starting volume (10) to begin extracting.\n",
    "# - 'number_of_volumes': The number of volumes (174) to extract, starting from 'start_vol'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion correction\n",
    "\n",
    "In FSL, we use <a href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/MCFLIRT\">MCFLIRT</a> to perform this correction.\n",
    "\n",
    "By default, MCFLIRT selects the middle volume of the EPI serie as reference to which other volumes are realigned.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If everything already set up, SKIP\n",
    "\n",
    "from fsl.wrappers import mcflirt\n",
    "\n",
    "dataset_id = 'ds000171'\n",
    "subject_id = 'control01' \n",
    "\n",
    "# Define the path where the dataset will be stored. Change this to your desired path.\n",
    "sample_path = \"/home/jovyan/Data/dataset\"\n",
    "\n",
    "mkdir_no_exist(sample_path)\n",
    "# Create the dataset directory if it does not already exist.\n",
    "\n",
    "bids_root = op.join(os.path.abspath(\"\"), sample_path, dataset_id)\n",
    "# Define the root directory for the BIDS dataset by combining the current working directory (absolute path),\n",
    "# the sample path, and the dataset ID.\n",
    "\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "# Define the directory for derivatives (processed files). It is located within the BIDS root.\n",
    "\n",
    "preproc_root = op.join(bids_root, 'derivatives', 'preprocessed_data')\n",
    "# Define the directory for preprocessed data, also within the derivatives folder.\n",
    "\n",
    "subprocess.run([\n",
    "    \"openneuro-py\", \"download\", \"--dataset\", dataset_id, \"--target-dir\", bids_root,\n",
    "    \"--include\", 'sub-{}'.format(subject_id)\n",
    "], check=True)\n",
    "# Download the dataset from OpenNeuro using the openneuro-py command.\n",
    "# - The dataset ID is specified with '--dataset'.\n",
    "# - The target directory for the download is set with '--target-dir', which is 'bids_root'.\n",
    "# - The '--include' flag ensures that only the specific subject (sub-001) is downloaded.\n",
    "# 'check=True' ensures the command raises an error if it fails.\n",
    "\n",
    "# Create necessary directories for the derivatives and preprocessed data\n",
    "mkdir_no_exist(deriv_root) # Create the derivatives directory if it doesn't exist.\n",
    "mkdir_no_exist(preproc_root) # Create the preprocessed data directory if it doesn't exist.\n",
    "mkdir_no_exist(os.path.join(preproc_root, 'sub-control01')) # Create the subject directory under preprocessed data.\n",
    "mkdir_no_exist(os.path.join(preproc_root, 'sub-control01', 'func')) # Create the functional data directory for the subject.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the path to the original BOLD data file in the BIDS directory structure\n",
    "path_original_data = os.path.join(bids_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold')\n",
    "\n",
    "# Define the path where the motion-corrected BOLD data will be saved\n",
    "path_moco_data = os.path.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_moco')\n",
    "\n",
    "mcflirt(\n",
    "    infile=path_original_data,  # The input file (original BOLD data)\n",
    "    o=path_moco_data,           # The output file (motion-corrected data)\n",
    "    plots=True,                 # Generate plots for visualizing motion correction.\n",
    "    report=True,                # Generate a report on the motion correction process.\n",
    "    dof=6,                      # Set degrees of freedom to 6 for rigid-body transformation.\n",
    "    mats=True                   # Save the transformation matrices.\n",
    ")\n",
    "# Perform motion correction using MCFLIRT from the FSL library:\n",
    "# - 'infile' specifies the input BOLD file to correct for motion.\n",
    "# - 'o' specifies the output file path for the motio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5) # Print the directory tree of the BIDS root directory up to a maximum depth of 5 levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the functional folder, notice that we have two new files:\n",
    "```\n",
    "sub-control01_task-music_run-1_bold_moco.nii.gz\n",
    "sub-control01_task-sitrep_run-1_bold_moco.par\n",
    "\n",
    "```\n",
    "\n",
    "The first one is the corrected EPI time serie, with volumes realigned. The second is a file describing the motion parameters that were used to move each volume. It will be useful very shortly to determine which volume moved by a lot.\n",
    "Notice as well a new directory!\n",
    "```\n",
    "sub-control01_task-sitrep_run-1_bold_moco.mat/\n",
    "```\n",
    "This directory is full of .MAT files. These are the transformation matrices used for every volume to realign them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays() # Reset the overlays in FSLeyes\n",
    "fsleyesDisplay.load(path_original_data) # Load the original data in FSLeyes\n",
    "fsleyesDisplay.load(path_moco_data) # Load the motion corrected data in FSLeyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motion parameters are stored in the .par file produced by MCFLIRT. Notice that since each volume moved differently, we have one transformation per volume, thus one set of motion parameters per volume as well. We provide you with a way to load these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mot_params_fsl_6_dof(path):\n",
    "    return pd.read_csv(path, sep='  ', header=None, \n",
    "            engine='python', names=['Rotation x', 'Rotation y', 'Rotation z','Translation x', 'Translation y', 'Translation z'])\n",
    "\n",
    "# Load the motion parameters from the motion corrected data. \n",
    "\n",
    "mot_params = load_mot_params_fsl_6_dof(op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_moco.par')) \n",
    "mot_params # It returns a dataframe with the motion parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on **translation on X alone**, can you find perhaps a volume which exceeds with respect to the **preceding volume** a 0.2 mm displacement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here to inspect quickly the translation on X :)\n",
    "#%matplotlib inline\n",
    "\n",
    "# Getting the translation is easy\n",
    "trans_x = mot_params[\"Translation x\"] # Getting the translation on X\n",
    "# Now, we want a 0.2mm with respect to previous frame:\n",
    "disp_x = np.diff(trans_x) # Getting the displacement on X. We use np.diff to calculate the difference between consecutive elements in the translation on X.\n",
    "# Lastly, we can ask for displacements (in absolute value) above 0.2mm and plot it to be clear:\n",
    "threshold=0.2 \n",
    "plt.plot(np.abs(disp_x))\n",
    "plt.hlines(threshold, 0, 370,colors='black', linestyles='dashed', label='FD threshold')\n",
    "plt.xlabel(\"Volumes\")\n",
    "plt.ylabel(\"Framewise translation displacement (mm)\")\n",
    "plt.show()\n",
    "\n",
    "# There are basically (if looking only along X translation) no frame displacement above 0.2mm! It means that the motion correction was very good for this subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the code below to extract an aggregate measure of motion for all volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_FD_power(mot_params):\n",
    "    framewise_diff = mot_params.diff().iloc[1:]\n",
    "\n",
    "    rot_params = framewise_diff[['Rotation x', 'Rotation y', 'Rotation z']]\n",
    "    # Estimating displacement on a 50mm radius sphere\n",
    "    # To know this one, we can remember the definition of the radian!\n",
    "    # Indeed, let the radian be theta, the arc length be s and the radius be r.\n",
    "    # Then theta = s / r\n",
    "    # We want to determine here s, for a sphere of 50mm radius and knowing theta. Easy enough!\n",
    "    \n",
    "    # Another way to think about it is through the line integral along the circle.\n",
    "    # Integrating from 0 to theta with radius 50 will give you, unsurprisingly, r0 theta.\n",
    "    converted_rots = rot_params*50  \n",
    "    trans_params = framewise_diff[['Translation x', 'Translation y', 'Translation z']]\n",
    "    fd = converted_rots.abs().sum(axis=1) + trans_params.abs().sum(axis=1)\n",
    "    return fd\n",
    "\n",
    "fd = compute_FD_power(mot_params).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.quantile(fd,0.75) + 1.5*(np.quantile(fd,0.75) - np.quantile(fd,0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "plt.plot(list(range(1, fd.size+1)), fd)\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('FD displacement (mm)')\n",
    "plt.hlines(threshold, 0, 370,colors='black', linestyles='dashed', label='FD threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay great, but what if we want to know which volumes are actually above threshold? Simply run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(fd > threshold)[0] + 1 # np.where() returns the indices of the elements that satisfy the condition. \n",
    "                                #We add 1 to the result to get the volume numbers instead of the indices. The [0] at the end is to get the array of indices instead of a tuple of arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion-correction: conclusions\n",
    "\n",
    "Motion correction should always be conducted. As you've seen, it is extremely easy to do and has many benefits. However it is not infaillible. High motion tends to cause non linear effects in the signal that simple motion correction above cannot correct since it has no awareness of the magnetic field. <br>\n",
    "<br> Motion parameters can, in this case, come to our rescue. As they represent the effect of motion, including them in our modeling to try and correct the signal can help. One could for example include this information in a General Linear Model to regress out the signal of these volumes (censoring) from overall timeseries. âž¡ï¸ More on this next week!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we?\n",
    "\n",
    "So, let's see what we have done so far:\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the B0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM in two weeks!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coregistration of functional to anatomical\n",
    "\n",
    "You have seen coregistration last week, when you were trying to align the T1 to MNI152, both manually and algorithmically. In the specific case of putting a T1 anatomical in a template space (such as MNI), we call it <b>normalization</b>, because we...Normalize it !\n",
    "\n",
    "You've also seen above with motion-correction coregistration of EPI volumes (functional data) to each other to correct motion.\n",
    "\n",
    "But what if you wanted to put the functional data overlayed on the anatomy, to know more precisely which parts of the brain are activated? (**coregistration between functional and anatomical**)\n",
    "\n",
    "Computing the fMRI space to anatomical transformation is precisely the goal of coregistration.\n",
    "<br><br>\n",
    "To do this step, we will use : <a href=\"https://web.mit.edu/fsl_v5.0.10/fsl/doc/wiki/FLIRT(2f)UserGuide.html#epi_reg\">epi_reg</a> ! \n",
    "\n",
    "### Using epi_reg to do the EPI registration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"warning\" style='background-color:#C1ECFA; color: #112A46; border-left: solid darkblue 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b> ðŸ’¡ Pay attention ! ðŸ’¡</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Make sure that the whole head T1 and the skull-stripped T1 have the same orientation.\n",
    "For example, if you ran fsl_anat to extract the brain (which is fine), FSL will change in the headers the orientation of the T1 before skull-stripping. As a consequence, the brain-extracted T1 no longer has the same orientation as the original T1. If you display them on top of each other, they are perfectly matched, but not from the perspective of the <b>headers</b>, which can play nasty tricks on you when performing coregistration.</p>\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if parallel acceleration is used in the EPI acquisition then the *effective echo spacing* is the actual echo spacing between acquired lines in k-space divided by the acceleration factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epi_reg has one peculiarity. If you launch it on a 4D volume, it will truncate your result to the first volume, because it expects a *single* EPI volume. We should thus first extract a single volume from our EPI, and then call epi_reg on it. We do that for you below.\n",
    "\n",
    "If you want to run with 4D volume to see the result and the warning, set use_single_vol to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsl.wrappers import epi_reg\n",
    "\n",
    "#################\n",
    "# Solution\n",
    "# We use the motion-corrected EPI\n",
    "##################\n",
    "\n",
    "# Define the path to the motion-corrected EPI (functional) image.\n",
    "epi_target = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_moco') # To change \n",
    "\n",
    "# Define the path to the whole (unprocessed) T1-weighted anatomical image.\n",
    "whole_t1 = op.join(bids_root, 'sub-control01', 'anat', 'sub-control01_T1w')\n",
    "\n",
    "# Define the path to the skull-stripped T1-weighted anatomical image.\n",
    "skull_stripped_t1 = op.join(preproc_root, 'sub-control01', 'anat', 'sub-control01_T1w') # ...\n",
    "\n",
    "# Define the path where the output of the epi_reg process will be saved.\n",
    "output_path = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_bbr') # ....\n",
    "\n",
    "# Define the path for the middle volume of the motion-corrected EPI file, which will be extracted as a reference.\n",
    "ref_vol_name = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_moco_vol_middle') \n",
    "\n",
    "# Set whether to use a single volume as a reference. This can be useful for registration purposes.\n",
    "use_single_vol = True\n",
    "\n",
    "if use_single_vol:\n",
    "    # Extract the middle volume of the EPI image using fslroi. This extracts a single volume from the time series.\n",
    "    # '182' is the index of the middle volume (assuming the total number of volumes is known). # To change \n",
    "    # '1' indicates that only one volume will be extracted.\n",
    "    fslroi(epi_target, ref_vol_name, str(), str(1))\n",
    "    \n",
    "    # Run epi_reg to register the extracted middle EPI volume to the T1-weighted anatomical image.\n",
    "    subprocess.run([\n",
    "        'epi_reg',\n",
    "        '--epi={}'.format(ref_vol_name),  # The EPI image (middle volume) to be registered.\n",
    "        '--t1={}'.format(whole_t1),       # The whole, non-skull-stripped T1-weighted image.\n",
    "        '--t1brain={}'.format(skull_stripped_t1),  # The skull-stripped T1-weighted image.\n",
    "        '--out={}'.format(output_path)    # The output path where the registered image will be saved.\n",
    "    ])\n",
    "else:\n",
    "    # If not using a single volume, register the entire motion-corrected EPI series to the T1-weighted anatomical image.\n",
    "    subprocess.run([\n",
    "        'epi_reg',\n",
    "        '--epi={}'.format(epi_target),    # The full motion-corrected EPI image (all volumes).\n",
    "        '--t1={}'.format(whole_t1),       # The whole, non-skull-stripped T1-weighted image.\n",
    "        '--t1brain={}'.format(skull_stripped_t1),  # The skull-stripped T1-weighted image.\n",
    "        '--out={}'.format(output_path)    # The output path where the registered image will be saved.\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how FAST is ran?\n",
    "This is because the specific coregistration cost (boundary-based registration, BBR) uses the **anatomical white-matter tissues from FAST**. If no such tissue is provided to the function, it re-runs FAST to obtain it and use it. If you've already done anatomical segmentation. \n",
    "\n",
    "If you had to yourself correct the white matter with the help of an expert because somehow FSL did a poor job on your data. Clearly you'd like to have this one used instead of the result from FAST, right?\n",
    "\n",
    "Well- you can! We just need a new option in the epi_reg command:\n",
    "```python\n",
    "epi_reg(...,wmseg=path_to_your_white_matter_segmentation)\n",
    "```\n",
    "Remember from last week, which T1 file corresponds to the white matter, between pve_0, pve_1 and pve_2 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Solution\n",
    "# White matter corresponds to pve_2.\n",
    "##############\n",
    "\n",
    "white_matter_segmentation = op.join(preproc_root, 'sub-control01', 'anat', 'sub-control01_T1w_fast_pve_2.nii.gz') # We provide the white matter segmentation\n",
    "\n",
    "subprocess.run(['epi_reg','--epi={}'.format(ref_vol_name), '--t1={}'.format(whole_t1), '--t1brain={}'.format(skull_stripped_t1), \n",
    "                '--out={}'.format(output_path),\n",
    "               '--wmseg={}'.format(white_matter_segmentation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overlay the two (EPI and anatomical) on top of each other to visualize the quality of the coregistration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(skull_stripped_t1)\n",
    "fsleyesDisplay.load(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is the functional in the right orientation?\n",
    "- Are the ventricles correctly aligned?\n",
    "- Are the boundaries of the EPI more or less matching the anatomical?\n",
    "\n",
    "âž¡ï¸ You can also check how the white matter of the EPI matches your anatomical's white matter provided you have sufficient resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some cleanup\n",
    "If you have a look, you might notice that perhaps your directory got filled with many files. These are temporary files, created but uncorrectly not eliminated by epi_reg. The following should help:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_epi_reg(path_to_clean):\n",
    "    patterns = ['*_fast_*', '*_fieldmap*']\n",
    "    for p in patterns:\n",
    "        files = glob.glob(op.join(path_to_clean, p))\n",
    "        for f in files:\n",
    "            os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_epi_reg(op.join(preproc_root, 'sub-control01', 'func'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these transforms are not exactly perfect. As you've seen in class, a step of smoothing is typically applied, with the size of the smoothing being dependent on your application, starting resolution etc.\n",
    "The idea of smoothing is really that, as you're averaging, hopefully you increase the signal to noise ratio. <br>\n",
    "A side-effect is that finest patterns of activation will be lost in the averaging (we can't have everything: there's no free lunch).\n",
    "\n",
    "With FSL, smoothing is rather easy to do. However, one thing which is important is the size of your filter.\n",
    "Different softwares might use different conventions. For MRI, it is typical to talk about FWHM (Full-width at half maximum), expressed in mms.\n",
    "\n",
    "FSL, however, takes as input in sigma instead of FWHM. The conversion is easy fortunately:\n",
    "\n",
    "$$ \\sigma = \\frac{FWHM}{2.3548}$$\n",
    "\n",
    "Here for example would be the smoothing command for 6mm FWHM smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where the output will be saved (same as input).\n",
    "# output_path = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_bbr') # ....\n",
    "\n",
    "cmd = 'fslmaths {} -s {} {}_smoothed-6mm'.format(output_path, 6/2.3548, output_path)\n",
    "# This line constructs a command string using 'fslmaths' to apply spatial smoothing to the image.\n",
    "# - 'fslmaths' is the FSL command used for mathematical manipulation of images.\n",
    "# - '{}': Placeholder for 'output_path', which is the path to the image that needs smoothing.\n",
    "# - '-s': This option applies a Gaussian kernel smoothing with a given standard deviation (sigma).\n",
    "# - '6/2.3548': The value (6 mm divided by 2.3548) converts the full width at half maximum (FWHM) to sigma.\n",
    "# - '{}_smoothed-6mm': This specifies the output file name, indicating that it has been smoothed with a 6 mm FWHM kernel.\n",
    "\n",
    "subprocess.run(['fslmaths', output_path, '-s', str(6/2.3548), '{}_smoothed-6mm'.format(output_path)])\n",
    "# This line runs the 'fslmaths' command directly as a subprocess.\n",
    "# - 'fslmaths': The command to manipulate the image.\n",
    "# - 'output_path': The input file path of the image to be smoothed.\n",
    "# - '-s': The option for applying Gaussian smoothing.\n",
    "# - 'str(6/2.3548)': The sigma value calculated by converting the 6 mm FWHM value to standard deviation.\n",
    "# - '{}_smoothed-6mm'.format(output_path)': The output path where the smoothed image will be saved, named with a suffix '_smoothed-6mm'.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe what we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(output_path + '_smoothed-6mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 MRI + fMRI preprocessing: summary\n",
    "\n",
    "So, these were all the steps you were meant to study this week. Next week, we'll present advanced steps of fMRI, but they are not always conducted unlike those we've shown you above which should always be considered.\n",
    "\n",
    "You should know by now: preprocessing is extremely important and you will likely spend a lot of time on it. Decisions in preprocessing will affect your analysis, so do not take this step lightly, it is <u>critical</u> to do it as well as possible!\n",
    "\n",
    "<u>Always perform quality control to ensure everything is okay!</u>\n",
    "\n",
    "Let's review one last time the different steps you've studied and which FSL tool(s) you used to do it:\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT + FNIRT (from last week), or ANTs</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the B0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM next week!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Coregistration to anatomical</td><td style='text-align:justify;'>Putting the functional volumes in anatomical space</td><td style='text-align:justify;'>FLIRT (epi_reg being a specialized instance)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Smoothing</td><td style='text-align:justify;'>Allowing a bit of lee-way in the voxel's values to account for the imperfection of the registration</td><td style='text-align:justify;'>fslmath with smoothing operation</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field map unwarping\n",
    "\n",
    "The field itself is not homogeneous. This means, in turn, that there are distortions in the acquisition.\n",
    "We can try to correct for it, through field maps, provided they've been acquired.\n",
    "\n",
    "### What are field maps ? \n",
    "\n",
    "Field maps are maps of the magnetic field (hence their name). They are acquired during an experimental session to capture parts where the MRI's magnetic field might present inhomogeneities. These inhomogeneities will, in turn, cause distortions in the signal which are not part of the subject's anatomy, as well as signal drop (places where the contrast becomes very small between tissues). Such artefacts should obviously be removed.\n",
    "This is where fieldmaps are typically coming into play: by knowing how your scanner is distorting your signal, you can hope to correct for it - to some amount.\n",
    "\n",
    "The first step is - naturally - to acquire fieldmaps. You will need to download them as we have avoided loading them for you - on purpose!\n",
    "\n",
    "To make sure you've understood how to load datasets, here is the dataset of interest: https://openneuro.org/datasets/ds004226/versions/1.0.0\n",
    "\n",
    "\n",
    "Your first task is to load:\n",
    "- Subject 001 data files, including the fieldmap files, located in the fmap subfolder (WITH the JSON sidecars!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If everything set up, SKIP it!\n",
    "\n",
    "import subprocess\n",
    "\n",
    "dataset_id = 'ds000171'\n",
    "subject_id = 'control01'\n",
    "\n",
    "sample_path = \"/home/jovyan/Data/dataset\"\n",
    "mkdir_no_exist(sample_path)\n",
    "bids_root = op.join(os.path.abspath(\"\"),sample_path, dataset_id)\n",
    "deriv_root = op.join(bids_root, 'derivatives')\n",
    "preproc_root = op.join(bids_root, 'derivatives','preprocessed_data')\n",
    "\n",
    "fmap_path = op.join(bids_root, 'sub-001', 'fmap') \n",
    "subject_dir = 'sub-{}'.format(subject_id)\n",
    "\n",
    "##################\n",
    "# Solution\n",
    "# There are two solutions\n",
    "# The easiest is simply to include all data of subject-01\n",
    "# The other is to add one line for the fieldmaps\n",
    "##################\n",
    "# Change the command below to include files in the fmap subdirectory\n",
    "# You should STILL be loading the EPI and anatomical\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, # Openneuro has for each dataset a unique identifier\n",
    "                \"--target-dir\", bids_root,  # The path where we want to save our data. You should save your data under /home/jovyan/Data/[your dataset ID] to be 100% fool-proof\n",
    "                \"--include\", op.join(subject_dir, 'anat','*'),# We are asking to get all files within the subject_dir/anat folder by using the wildcard *\n",
    "                \"--include\", op.join(subject_dir, 'func','*'),# We are asking to get all files within the subject_dir/func folder by using the wildcard *\n",
    "                \"--include\", op.join(subject_dir, 'fmap','*'),# We are asking to get all files within the subject_dir/fmap folder by using the wildcard *\n",
    "               ], check=True)\n",
    "\n",
    "# Simple variant to include everything from subject-01\n",
    "subprocess.run([\"openneuro-py\", \"download\", \"--dataset\", dataset_id, # Openneuro has for each dataset a unique identifier\n",
    "                \"--target-dir\", bids_root,  # The path where we want to save our data. You should save your data under /home/jovyan/Data/[your dataset ID] to be 100% fool-proof\n",
    "                \"--include\", subject_dir # Effectively get all data\n",
    "               ], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, remember this download might not be finished immediately. Now, assuming it *is*, let's have a look at what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look in the fmap folder. There are four files, corresponding to two fieldmap acquisitions. One is PA, the other is AP.\n",
    "\n",
    "We need some parameters to be able to exploit these files.\n",
    "In particular, we need to figure out:\n",
    "- The phase encoding direction\n",
    "- The total readout time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_json_from_file(op.join(fmap_path, 'sub-control01_acq-task_dir-{}_epi.json'.format('AP')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If everything is already set up, skip it!\n",
    "\n",
    "mkdir_no_exist(deriv_root)\n",
    "mkdir_no_exist(preproc_root)\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-control01'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-control01', 'func'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-control01', 'anat'))\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-control01', 'fmap'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_fmap_path = op.join(preproc_root, 'sub-control01', 'fmap')\n",
    "# Define the path for storing the preprocessed fieldmap data for the subject 'sub-001' within the preprocessed data directory.\n",
    "\n",
    "mkdir_no_exist(preproc_fmap_path)\n",
    "# Create the directory if it doesn't already exist. This directory will hold the files related to the fieldmaps.\n",
    "\n",
    "direction_file = op.join(preproc_fmap_path, 'datain.txt')\n",
    "# Define the path for the 'datain.txt' file, which will store information about the phase encoding directions and total readout times.\n",
    "\n",
    "f = open(direction_file, 'w')\n",
    "# Open the 'datain.txt' file in write mode. This file will be used to record phase encoding direction information for the fieldmaps.\n",
    "\n",
    "for name in ['AP', 'PA']:\n",
    "    # Iterate over the two phase encoding directions: 'AP' (Anterior-Posterior) and 'PA' (Posterior-Anterior).\n",
    "    \n",
    "    data = get_json_from_file(op.join(fmap_path, 'sub-control01_acq-task_dir-{}_epi.json'.format(name)))\n",
    "    # Read the JSON file for the fieldmap corresponding to the current direction ('AP' or 'PA').\n",
    "    # These JSON files contain metadata about the fieldmaps, such as phase encoding direction and total readout time.\n",
    "    \n",
    "    phase_dir = data['PhaseEncodingDirection'] # Extract the phase encoding direction from the JSON data.\n",
    "    total_readout_time = data['TotalReadoutTime'] # Extract the total readout time from the JSON data.\n",
    "\n",
    "    # We expect a specific format: x y z total_readout_time.\n",
    "    # The values x, y, and z are set to 1/-1 if they match the phase encoding direction, otherwise they are set to 0.\n",
    "    phase = [0, 0, 0, total_readout_time] # Initialize the phase list with zeros for x, y, and z, and the total readout time as the fourth element.\n",
    "    \n",
    "    is_neg = len(phase_dir) == 2 and phase_dir[1] == '-'\n",
    "    # Check if the phase encoding direction is negative (e.g., 'j-' or 'i-'). If it is negative, set 'is_neg' to True.\n",
    "\n",
    "    phase_dir = phase_dir[0]\n",
    "    # Extract the first character ('i', 'j', or 'k') from the phase encoding direction, which represents the axis (x, y, or z).\n",
    "\n",
    "    phase[ord(phase_dir) - ord('i')] = -1 if is_neg else 1\n",
    "    # Determine which axis ('i' for x, 'j' for y, or 'k' for z) the phase encoding applies to by subtracting the ASCII value of 'i' from 'phase_dir'.\n",
    "    # Set the corresponding element in the phase list to -1 if it's a negative direction, or 1 otherwise.\n",
    "\n",
    "    for i in range(3):\n",
    "        f.write('{} {} {} {}\\n'.format(phase[0], phase[1], phase[2], phase[3]))\n",
    "        # Write the phase encoding information to the 'datain.txt' file in the format: x y z total_readout_time.\n",
    "        # Repeat this three times for each phase encoding direction ('AP' and 'PA').\n",
    "\n",
    "f.close()\n",
    "# Close the 'datain.txt' file after writing all the information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the field map\n",
    "Now, we will create the field map.\n",
    "This process is tedious, sometimes hard to get right. First, let's look at the two fieldmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(fmap_path, 'sub-control01_acq-task_dir-AP_epi.nii.gz'))\n",
    "fsleyesDisplay.load(op.join(fmap_path, 'sub-control01_acq-task_dir-PA_epi.nii.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you notice, they are quite different with respect to their distortions. This is because they used different phase encoding directions (Anterior -> Posterior and Posterior -> Anterior, hence AP and PA).\n",
    "\n",
    "Looking from these two encoding directions, we (or rather clever algorithm: <a href=\"https://web.mit.edu/fsl_v5.0.10/fsl/doc/wiki/topup.html\">topup</a>) can build a complete map of the distortions on our EPI / fMRI file.\n",
    "\n",
    "First, AP and PA fieldmaps should be made a single file to be fed to topup.\n",
    "- You also need to feed it the direction file we created above (which specifies encoding direction and readout time of our EPI).\n",
    "- From the computed field, we need to convert it to radians, and finally, we obtain both a phase information for the field - in radians - and a magnitude information.\n",
    "\n",
    "You are now ready to apply the fieldmap to correct distortions.\n",
    "\n",
    "The function below conducts all these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"topup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall:  direction_file = op.join(preproc_fmap_path, 'datain.txt')\n",
    "\n",
    "def generate_fmap_AP_PA(direction_file):\n",
    "    \"\"\"\n",
    "    From an AP/PA pair of files, generate the corresponding fieldmap files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    direction_file: str\n",
    "        Path to a direction file (typically called datain.txt) indicating phase encoding direction, total \n",
    "        readout time, and other relevant parameters for fieldmap computation.\n",
    "    \"\"\"\n",
    "    merged_phase_imgs = op.join(preproc_fmap_path, 'sub-control01_acq-task_dir-fmap_merged')\n",
    "    # Define the path for the merged AP and PA phase images. This file will store the combined EPI images from both directions.\n",
    "\n",
    "    # Step 1: Combine AP and PA as a single file\n",
    "    subprocess.run(['fslmerge', '-t', merged_phase_imgs, \n",
    "                    op.join(fmap_path, 'sub-control01_acq-task_dir-AP_epi.nii.gz'), \n",
    "                    op.join(fmap_path, 'sub-control01_acq-task_dir-PA_epi.nii.gz')])\n",
    "    # Merge the AP and PA phase images using 'fslmerge'. \n",
    "    # - '-t' specifies merging along the time axis (creates a 4D file with both volumes).\n",
    "    # - The output file is 'merged_phase_imgs', which combines the AP and PA files.\n",
    "\n",
    "    # Step 2: Compute the fieldmap deformation with topup\n",
    "    output_fmap = op.join(preproc_fmap_path, 'fieldmap_ex')\n",
    "    # Define the path for the output fieldmap (deformation map).\n",
    "\n",
    "    unwarped_img = op.join(preproc_fmap_path, 'se_epi_unwarped')\n",
    "    # Define the path for the output unwarped EPI image after correction.\n",
    "\n",
    "    subprocess.run(['topup', \n",
    "                    '--imain={}'.format(merged_phase_imgs), \n",
    "                    '--datain={}'.format(direction_file),\n",
    "                    '--config={}'.format('b02b0.cnf'),\n",
    "                    '--fout={}'.format(output_fmap),\n",
    "                    '--iout={}'.format(unwarped_img),\n",
    "                    '-v'])\n",
    "    # Run the 'topup' command to calculate the deformation field for the fieldmap.\n",
    "    # - '--imain': Specifies the input merged AP and PA EPI images.\n",
    "    # - '--datain': The direction file containing phase encoding directions and readout times ('datain.txt').\n",
    "    # - '--config': The configuration file ('b02b0.cnf') specific for fieldmap computation.\n",
    "    # - '--fout': The output deformation field file ('output_fmap').\n",
    "    # - '--iout': The output unwarped image ('unwarped_img').\n",
    "    # - '-v': Enables verbose output for detailed logs.\n",
    "\n",
    "    # Step 3: Convert fieldmap units to radians\n",
    "    subprocess.run(['fslmaths', output_fmap, '-mul', str(6.28), output_fmap + '_rads'])\n",
    "    # Convert the fieldmap values to radians using 'fslmaths'.\n",
    "    # - Multiply the fieldmap values by 2 * pi (6.28) to convert the output into radians.\n",
    "    # - Save the result as 'output_fmap_rads'.\n",
    "\n",
    "    # Step 4: Create a magnitude fieldmap\n",
    "    subprocess.run(['fslmaths', unwarped_img, '-Tmean', output_fmap + '_mag'])\n",
    "    # Create a magnitude image from the unwarped EPI image using 'fslmaths'.\n",
    "    # - '-Tmean' computes the mean across the time axis (if the image is 4D).\n",
    "    # - The result is saved as 'output_fmap_mag'.\n",
    "\n",
    "    # Extract fieldmap brain using BET (Brain Extraction Tool)\n",
    "    subprocess.run(['bet', output_fmap + '_mag', output_fmap + '_mag_brain', '-m', '-R'])\n",
    "    # Run 'bet' to extract the brain from the magnitude image.\n",
    "    # - The input is 'output_fmap_mag', and the output is 'output_fmap_mag_brain'.\n",
    "    # - '-m' outputs a brain mask as well.\n",
    "    # - '-R' uses a robust brain center estimation.\n",
    "\n",
    "# Call the function to generate the fieldmap using the specified direction file\n",
    "generate_fmap_AP_PA(direction_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It now remains to apply the fieldmap! \n",
    "\n",
    "To do so we will apply to the **first volume of our series to show you the result of distortion correction**.\n",
    "Therefore, extract the first volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_trim = op.join(bids_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold.nii.gz')\n",
    "# This line creates the file path to the specific EPI (fMRI) volume we want to trim. \n",
    "# It uses the 'op.join' function to concatenate the root directory ('bids_root') with the sub-directory and file name.\n",
    "# The file ('sub-control01_task-music_run-1_bold.nii.gz') is located in the 'func' folder for subject 'sub-control01'.\n",
    "\n",
    "mkdir_no_exist(op.join(preproc_root, 'sub-control01', 'func'))\n",
    "# This line creates a new directory in the preprocessed root path ('preproc_root') for the same subject ('sub-control01') if it does not already exist.\n",
    "# The new directory will store the processed files. The function 'mkdir_no_exist' ensures the directory is created only if it doesn't already exist.\n",
    "\n",
    "extracted_epi = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run_1_bold_vol_1')\n",
    "# This line sets the file path for the output volume after extraction. \n",
    "# It uses 'op.join' to concatenate 'preproc_root' with the necessary sub-directory and file name where the first volume will be saved.\n",
    "# The output file is named 'sub-control01_task-music_run_1_bold_vol_1'.\n",
    "\n",
    "###########\n",
    "# Solution\n",
    "# Extracting the first volume means we keep a single volume. \n",
    "# This is exactly as we did last week for the reference EPI used in epi_reg,\n",
    "# but we take the volume number 0 instead of the middle one.\n",
    "###########\n",
    "\n",
    "# Select only the FIRST volume!\n",
    "start_vol = 0 # Where should we start? (First volume is 0, not 1!)\n",
    "# This variable defines the starting volume index for extraction. In fMRI data, indexing typically starts from 0.\n",
    "# Here, '0' indicates that we want to extract the very first volume in the 4D EPI data.\n",
    "\n",
    "number_of_volumes = 1 # How many volumes should we keep?\n",
    "# This variable defines the number of volumes to extract from the dataset. \n",
    "# Setting this to '1' means that only the first volume will be extracted.\n",
    "\n",
    "fslroi(file_to_trim, extracted_epi, str(start_vol), str(number_of_volumes))\n",
    "# This line calls 'fslroi', a command-line tool from FSL (FMRIB Software Library), to extract a specific subset of volumes from the 4D EPI file.\n",
    "# 'file_to_trim' is the input EPI file, 'extracted_epi' is the path where the extracted volume will be saved.\n",
    "# The function uses 'start_vol' to determine where to start the extraction and 'number_of_volumes' to determine how many volumes to keep.\n",
    "# This command extracts the first volume (volume 0) from the EPI data and saves it as a new file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(extracted_epi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally able to apply our fieldmap to the EPI. We can do so, using FUGUE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmap_path_rad = op.join(preproc_root, 'sub-control01', 'fmap', 'fieldmap_ex_rads')\n",
    "# Define the path to the fieldmap file in radians. This file was created in a previous step and is stored in the 'fmap' directory for subject 'sub-001'.\n",
    "\n",
    "epi_result = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_vol_1_fmap')\n",
    "# Define the output path for the unwarped EPI volume. The result will be saved as 'sub-001_task-sitrep_run-01_bold_vol_1_fmap' in the 'func' directory.\n",
    "\n",
    "unwarpdir = 'y-'\n",
    "# Define the direction for the unwarping. Here, 'y-' indicates that the phase encoding direction is along the negative y-axis.\n",
    "\n",
    "dwell_time = data['EffectiveEchoSpacing']\n",
    "# Extract the dwell time (Effective Echo Spacing) from the dataset's metadata.\n",
    "# Note: The actual dwell time in the image header might be incorrect (reported in microseconds instead of milliseconds). \n",
    "# The 'EffectiveEchoSpacing' provides the correct magnitude and is used instead.\n",
    "\n",
    "# Apply the fieldmap correction using FUGUE (FSL utility for unwarping EPI images):\n",
    "subprocess.run([\n",
    "    'fugue', \n",
    "    '-i', extracted_epi,  # The input EPI volume (the one that will be unwarped).\n",
    "    '--loadfmap={}'.format(fmap_path_rad),  # Load the fieldmap (in radians) generated earlier.\n",
    "    '--dwell={}'.format(dwell_time),  # Set the dwell time parameter for accurate unwarping.\n",
    "    '--unwarpdir={}'.format(unwarpdir),  # Specify the direction of phase encoding for unwarping.\n",
    "    '-u', epi_result  # The output path for the unwarped EPI image.\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.load(epi_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='margin-top:1em; text-align:center'><b>ðŸ’¡ Pay attention! ðŸ’¡</b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    Field maps for this dataset came in a specific format. But they can come in <b>many</b> different ways, meaning you will need to be very careful when recovering them. The steps outlined above in particular are only applicable in the case of having an AP-PA acquisition. Here is the full resource of FSL's FUGUE on field map unwarping: <a href=https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/fugue>https://fsl.fmrib.ox.ac.uk/fsl/docs/#/registration/fugue</a> . Don't be afraid to refer to it, should you have a different format in a project!</p>\n",
    "</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining transforms\n",
    "\n",
    "We know how to perform motion-correction, and how to coregister an image to another. That's great!\n",
    "But each transformation usually implies a step of interpolation (because the image is transformed and must be resampled). This interpolation means the resulting data is \"corrupted\" slightly. We would like to minimize the amount of interpolations to only once if possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of transformations we would like to have is:\n",
    "EPI â†’ Motion correction (Mcflirt) â†’ EPI (motion corrected) â†’ Field unwarping + affine coregistration (epi_reg with fieldmap) â†’ EPI (Anatomical space) â†’ Normalization (Flirt or ANTs) â†’ EPI (template space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining fieldmap unwarping and EPI registration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FSL provides a way to compute the EPI to anatomical while combining it with the fieldmap unwarping. \n",
    "\n",
    "But before applying all transforms, let's worry about doing the required preprocessing, namely:\n",
    "- Motion correction\n",
    "- Field unwarping\n",
    "- EPI to anatomical coregistration\n",
    "- Anatomical to template coregistration (Normalization): we will use the MNI152 1mm template that you know from the previous weeks\n",
    "\n",
    "First, remark that motion correction is done by selecting a reference volume in the EPI to which all others are coregistered. By default, the middle EPI was used. Because we used in our fieldmap computation the first EPI, we need to use this one instead.\n",
    "\n",
    "**It is critical that you pay attention to which image was used to compute your transformations, otherwise combining them won't make sense!**. For this reason, let's now go over the entire pipeline and transformation steps, sticking to the first EPI. We extract it again with fslroi, and we re-run the motion correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_epi = op.join(bids_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold')\n",
    "# Define the path to the original EPI (BOLD) image for subject 'sub-001'.\n",
    "# This file is located in the 'func' directory under the BIDS root directory.\n",
    "\n",
    "reference_epi = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_first-vol')\n",
    "# Define the output path for the extracted first volume of the EPI image.\n",
    "# This file will be saved in the preprocessed directory under the same 'func' directory as the original EPI file.\n",
    "\n",
    "fslroi(original_epi, reference_epi, str(0), str(1))\n",
    "# Use the FSL tool 'fslroi' to extract the first volume from the original EPI image:\n",
    "# - 'original_epi': The path to the input EPI image (the full 4D dataset).\n",
    "# - 'reference_epi': The output path where the first volume will be saved.\n",
    "# - 'str(0)': The index of the volume to start extracting from (0 indicates the first volume).\n",
    "# - 'str(1)': The number of volumes to extract (1 means only the first volume will be extracted).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do **motion correction**. Recall that it is done on the **entire** EPI timeseries with mcflirt. We will explicitly give the first epi as reference this time around, to force FSL to use this volume and realign everyone to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fsl.wrappers import mcflirt\n",
    "\n",
    "# Define the path for the motion-corrected output file.\n",
    "path_moco_data = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_moco')\n",
    "# This is where the motion-corrected EPI (BOLD) image will be saved after running MCFLIRT.\n",
    "# The file will be stored in the 'func' directory within the preprocessed data directory for subject 'sub-001'.\n",
    "\n",
    "# Run the MCFLIRT command to perform motion correction on the EPI image.\n",
    "mcflirt(\n",
    "    infile=original_epi,          # The input 4D EPI dataset that needs motion correction.\n",
    "    o=path_moco_data,             # The output file path where the motion-corrected data will be saved.\n",
    "    plots=True,                   # Generate motion correction plots for quality assessment.\n",
    "    report=True,                  # Generate a report that summarizes the motion correction process.\n",
    "    dof=6,                        # Use 6 degrees of freedom (rigid-body motion correction: 3 translations + 3 rotations).\n",
    "    mats=True,                    # Save the transformation matrices for each volume.\n",
    "    reffile=reference_epi         # Use the first volume extracted (reference_epi) as the reference image for motion correction.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! Now, because **the reference volume did not move at all** (since it is the reference to which everyone is realigned), we can use this volume as starting point to compute our other transforms: we're only missing the coregistration with fieldmap unwarping, as the normalization is obtained through the anatomical data :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 'control'\n",
    "subject = 'sub-control01'\n",
    "\n",
    "# Relevant paths for anatomical preprocessing\n",
    "anatomical_path = op.join(bids_root, subject, 'anat', 'sub-{}_T1w.nii.gz'.format(subject_id))\n",
    "# Path to the original T1-weighted anatomical image for the subject.\n",
    "\n",
    "betted_brain_path = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w'.format(subject_id))\n",
    "# Path where the skull-stripped version of the anatomical image will be saved after using BET.\n",
    "\n",
    "segmentation_path = op.join(preproc_root, 'sub-control01', 'anat', 'sub-control01_T1w_fast')\n",
    "# Path where the segmented brain tissues (gray matter, white matter, etc.) will be saved using FAST.\n",
    "\n",
    "mni_template = op.expandvars(op.join('$FSLDIR', 'data', 'standard', 'MNI152_T1_1mm_brain'))\n",
    "# Path to the MNI152 template brain image. This is used for normalization.\n",
    "\n",
    "anat_result = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w_mni'.format(subject_id))\n",
    "# Path where the normalized anatomical image (aligned to MNI space) will be saved.\n",
    "\n",
    "anat_2_mni_trans = op.join(preproc_root, subject, 'anat', 'sub-{}_T1w_2_mni_lin.mat'.format(subject_id))\n",
    "# Path where the linear transformation matrix from the subject's anatomical space to MNI space will be saved.\n",
    "\n",
    "# Relevant variables for epi_reg\n",
    "output_path = op.join(preproc_root, 'sub-001', 'func', 'sub-control01_task-music_run-1_bold_anat-space')\n",
    "# Path where the EPI image registered to anatomical space will be saved.\n",
    "\n",
    "dwell_time = 0.000620007\n",
    "# Dwell time for the EPI image, which is needed for fieldmap unwarping.\n",
    "\n",
    "unwarpdir = 'y-'\n",
    "# Unwarping direction for fieldmap correction, indicating the phase encoding direction.\n",
    "\n",
    "##############\n",
    "# Steps needed before using epi_reg:\n",
    "# - Brain extraction with BET\n",
    "# - Segmentation with FAST\n",
    "# - Normalization to MNI152 space with FLIRT\n",
    "##############\n",
    "\n",
    "from fsl.wrappers import fast\n",
    "\n",
    "# Perform brain extraction using BET\n",
    "subprocess.run(['bet', anatomical_path, betted_brain_path, '-m', '-R'])\n",
    "# 'bet': Brain extraction tool\n",
    "# 'anatomical_path': Input anatomical image\n",
    "# 'betted_brain_path': Output skull-stripped image\n",
    "# '-m': Generate a brain mask\n",
    "# '-R': Robust estimation of brain center\n",
    "\n",
    "# Perform tissue segmentation using FAST\n",
    "fast(imgs=[betted_brain_path], out=segmentation_path, n_classes=3)\n",
    "# FAST is used for segmentation of the skull-stripped image\n",
    "# 'imgs': Input image (skull-stripped)\n",
    "# 'out': Output path for segmentation results\n",
    "# 'n_classes': Number of tissue classes (3: gray matter, white matter, CSF)\n",
    "\n",
    "# Normalize the skull-stripped anatomical image to MNI space using FLIRT\n",
    "flirt(betted_brain_path, mni_template, out=anat_result, omat=anat_2_mni_trans)\n",
    "# 'flirt': Linear registration tool\n",
    "# 'betted_brain_path': Skull-stripped anatomical image\n",
    "# 'mni_template': MNI template for alignment\n",
    "# 'out': Output path for the aligned anatomical image\n",
    "# 'omat': Output path for the transformation matrix\n",
    "\n",
    "#############\n",
    "# Perform EPI-to-anatomical registration with fieldmap correction using epi_reg\n",
    "#############\n",
    "reference_epi = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_first-vol')\n",
    "# Path for the first volume of the EPI image, used as a reference for registration.\n",
    "\n",
    "# Extract the first volume of the EPI dataset\n",
    "fslroi(original_epi, reference_epi, str(0), str(1))\n",
    "# 'fslroi': Extracts a single volume from the 4D EPI image\n",
    "# 'original_epi': Input EPI image\n",
    "# 'reference_epi': Output path for the first volume\n",
    "# 'str(0)': Index of the first volume (0-based)\n",
    "# 'str(1)': Number of volumes to extract (1)\n",
    "\n",
    "# Run epi_reg to perform EPI-to-anatomical registration with fieldmap unwarping\n",
    "subprocess.run([\n",
    "    'epi_reg',\n",
    "    '--epi={}'.format(reference_epi),  # Input EPI volume\n",
    "    '--t1={}'.format(anatomical_path),  # Input T1-weighted image\n",
    "    '--t1brain={}'.format(betted_brain_path),  # Skull-stripped T1-weighted image\n",
    "    '--out={}'.format(output_path),  # Output path for the registered EPI image\n",
    "    '--fmap={}'.format(op.join(preproc_root, 'sub-control01', 'fmap', 'fieldmap_ex_rads')),  # Fieldmap in radians\n",
    "    '--fmapmagbrain={}'.format(op.join(preproc_root, 'sub-control01', 'fmap', 'fieldmap_ex_mag_brain')),  # Skull-stripped magnitude image\n",
    "    '--fmapmag={}'.format(op.join(preproc_root, 'sub-control01', 'fmap', 'fieldmap_ex_mag')),  # Magnitude image of the fieldmap\n",
    "    '--wmseg={}'.format(op.join(preproc_root, 'sub-control01', 'anat', 'sub-control01_T1w_fast_pve_2')),  # White matter segmentation\n",
    "    '--echospacing={}'.format(dwell_time),  # Echo spacing (dwell time) for fieldmap correction\n",
    "    '--pedir={}'.format(unwarpdir)  # Phase encoding direction\n",
    "])\n",
    "\n",
    "print(\"Done with EPI to anatomical registration with fieldmap unwarping\")\n",
    "\n",
    "###### Note:  the code works on a single volume of the EPI dataset. ######\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The code performs several key preprocessing steps for aligning and correcting functional (EPI) MRI images with anatomical (T1-weighted) MRI images:\n",
    "### Anatomical Preprocessing:\n",
    "- Skull-Stripping: Uses BET (Brain Extraction Tool) to remove non-brain tissue from the T1-weighted anatomical image.\n",
    "- Segmentation: Applies FAST (FMRIB's Automated Segmentation Tool) to segment the skull-stripped image into different tissue types (e.g., gray matter, white matter, CSF).\n",
    "- Normalization: Uses FLIRT (FMRIB's Linear Image Registration Tool) to align the anatomical image to the MNI152 standard space, saving the transformation matrix for potential later use.\n",
    "### EPI Preprocessing:\n",
    "- Extracts the first volume from the 4D EPI dataset to use as a reference for alignment.\n",
    "- Applies epi_reg to register the EPI image to the anatomical space using the skull-stripped T1 image. It also incorporates fieldmap correction to correct for distortions based on phase encoding direction and echo spacing information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âž¡ï¸ Inspect the two resulting files to ensure that nothing went wrong. In other words:\n",
    "- Check that the MCFLIRT result is okay with respect to motion\n",
    "- Check that the realignment following epi_reg made the EPI well aligned with the anatomical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the saved transformation\n",
    "\n",
    "Applying the transformation to a single volume is nice, but we should still need to know where the transformation was saved, to apply it to all other volumes of interest.\n",
    "\n",
    "Let's inspect our resulting folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root,max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file <b>sub-001_task-sitrep_run-01_bold_anat-space_warp</b> corresponds to the <u>transformation</u> from EPI to anatomical file with the fieldmap unwarping applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the transforms\n",
    "\n",
    "Let's list our available transforms as is:\n",
    "\n",
    " <table>\n",
    "  <tr>\n",
    "    <th>Transform</th>\n",
    "    <th>Step</th>\n",
    "    <th>File(s)</th>\n",
    "    <th>Linear?</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Each EPI to reference EPI</td>\n",
    "    <td>Motion correction (mcflirt) </td>\n",
    "    <td>matrices in func/sub-001_task-sitrep_run-01_bold_moco.mat/</td>\n",
    "    <td>Yes</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>EPI to anat (with fieldmap)</td>\n",
    "    <td>Functional to anat coregistration (epi_reg) </td>\n",
    "    <td>func/sub-001_task-sitrep_run-01_bold_anat-space_warp.nii.gz</td>\n",
    "    <td>No</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Anat to template</td>\n",
    "    <td>Normalization (FLIRT or FNIRT or ANTs)</td>\n",
    "    <td>anat/.mat</td>\n",
    "    <td>Yes if FLIRT or ANTs with linear transform, no otherwise</td>\n",
    "  </tr>\n",
    "</table> \n",
    "\n",
    "We will now combine all these transformations, so that we apply one transformation and exactly one interpolation at the end to minimize errors.\n",
    "\n",
    "We provide you with a function to do so just below, along with a function to apply the resulting combined transformation. Beware, all these functions operate on single volumes, not on 4D data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_transforms(reference_volume, warp_save_name, is_linear, epi_2_moco=None, epi_2_anat_warp=None, anat_2_standard_warp=None):\n",
    "    \"\"\"\n",
    "    Combines transformations from motion correction to standard space transformation. \n",
    "    \n",
    "    Note: the code operates on a single volume per time \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_volume: str\n",
    "        The reference volume that will define the resolution and field of view for the final image after all transformations are applied.\n",
    "    warp_save_name: str\n",
    "        The filename where the combined transformation warp will be saved.\n",
    "    is_linear: bool\n",
    "        Indicates whether the final transformation is linear or non-linear.\n",
    "    epi_2_moco: str\n",
    "        The motion correction transformation matrix for the EPI volume (located in the .mat folder of the EPI).\n",
    "    epi_2_anat_warp: str\n",
    "        The transformation that aligns the EPI volume to the anatomical space, typically including fieldmap correction (non-linear).\n",
    "    anat_2_standard_warp: str\n",
    "        The transformation from the anatomical space to standard space (e.g., MNI152). This may be linear or non-linear.\n",
    "\n",
    "    \"\"\"\n",
    "    from fsl.wrappers import convertwarp\n",
    "    # Import the FSL function 'convertwarp' to combine transformations.\n",
    "\n",
    "    # Initialize the base arguments for convertwarp using the available transformations.\n",
    "    args_base = {'premat': epi_2_moco, 'warp1': epi_2_anat_warp}\n",
    "    # 'premat' is used for the motion correction transformation matrix.\n",
    "    # 'warp1' is the warp field that aligns the EPI to anatomical space.\n",
    "\n",
    "    if is_linear:\n",
    "        # If the transformation to standard space is linear, use 'postmat' for the transformation matrix.\n",
    "        args_base['postmat'] = anat_2_standard_warp\n",
    "    else:\n",
    "        # If the transformation to standard space is non-linear, use 'warp2' for the warp field.\n",
    "        args_base['warp2'] = anat_2_standard_warp\n",
    "\n",
    "    # Filter out any None values from the arguments, keeping only the valid transformations.\n",
    "    args_filtered = {k: v for k, v in args_base.items() if v is not None}\n",
    "\n",
    "    # Combine the transformations using convertwarp.\n",
    "    convertwarp(warp_save_name, reference_volume, **args_filtered)\n",
    "    # The combined transformation is saved as 'warp_save_name', with 'reference_volume' defining the output resolution.\n",
    "\n",
    "    print(\"Done with warp conversion\")\n",
    "    # Print a message indicating that the warp conversion is complete.\n",
    "\n",
    "def apply_transform(reference_volume, target_volume, output_name, transform):\n",
    "    \"\"\"\n",
    "    Applies a warp field to a target volume and resamples it to match the space of the reference volume.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reference_volume: str\n",
    "        The reference volume used for final interpolation, resampling, and setting the field of view.\n",
    "    target_volume: str\n",
    "        The target volume to which the warp will be applied.\n",
    "    output_name: str\n",
    "        The filename where the transformed image will be saved.\n",
    "    transform: str\n",
    "        The filename of the warp (assumed to be a .nii.gz file).\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    combine_all_transforms: To see how to build a warp field.\n",
    "    \"\"\"\n",
    "    from fsl.wrappers import applywarp\n",
    "    # Import the FSL function 'applywarp' to apply the transformation.\n",
    "\n",
    "    # Apply the warp transformation to the target volume, using the reference volume for resampling.\n",
    "    applywarp(target_volume, reference_volume, output_name, w=transform, rel=False)\n",
    "    # 'target_volume': The EPI or image to be transformed.\n",
    "    # 'reference_volume': Defines the resolution and space for resampling.\n",
    "    # 'output_name': The file path where the transformed volume will be saved.\n",
    "    # 'w=transform': Specifies the warp field to apply.\n",
    "    # 'rel=False': Ensures the warp is applied in absolute coordinates rather than relative.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- **combine_all_transforms**: Merges different transformation steps (motion correction, EPI-to-anatomical alignment, and anatomical-to-standard alignment) into a single warp file. This allows you to efficiently apply all transformations in one step.\n",
    "\n",
    "- **apply_transform**: Uses the combined warp to transform and resample a target image (e.g., EPI data) so it matches the reference space (e.g., the standard MNI space or the subjectâ€™s anatomical space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In combine_all_transforms, setting any transform to None instead of the correct transform will skip the transform step in the total transformation. This way, you should be able to perform quality control. In particular, please ensure that:\n",
    "- [ ] Applying ONLY motion correction transformation to the first volume yields the expected alignement (so it should be aligned with the \\_moco volume.)\n",
    "- [ ] Applying motion correction + epi -> anat should be aligned to anatomical\n",
    "- [ ] Finally, applying motion correction + epi > anat + anat > standard should be aligned to the standard\n",
    "Only once you're convinced these steps are working well should you proceed to standard space. **Remember the 1-10-100 rule! Always perform QC before moving on.**\n",
    "\n",
    "To help you, we provide you below with the template to do such a thing, so that you don't have to worry too much about the nitty gritty details.\n",
    "Focus on:\n",
    "- The reference file to use \n",
    "- The transformations to provide (either a file or None)\n",
    "\n",
    "<b>Notice this is performed only on a single volume. Indeed, if you are debugging you should avoid wasting time applying transformations on entire timeseries to quickly diagnose whether a step is working or failing.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fsl.wrappers import applywarp\n",
    "\n",
    "ref = mni_template\n",
    "# 'ref' is set to the MNI template, which will be used as the reference space for alignment.\n",
    "\n",
    "# The first volume of the EPI dataset is selected as the target.\n",
    "target_epi = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_first-vol')\n",
    "# This variable points to the path of the first volume extracted from the EPI time series.\n",
    "\n",
    "split_nbr = '0000'\n",
    "# A variable to keep track of the volume number being processed. In this case, it's the first volume ('0000').\n",
    "\n",
    "# We define the path where the combined warp will be saved.\n",
    "warp_name = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_split' + split_nbr + '_epi_2_std_warp')\n",
    "# This is where the combined warp (transformation) file for the EPI volume will be saved.\n",
    "\n",
    "# Define paths to the necessary transformations:\n",
    "\n",
    "# The EPI to anatomical warp file (with fieldmap corrections included).\n",
    "func_2_anat = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-sitrep_run-1_bold_anat-space_warp.nii.gz')\n",
    "\n",
    "# The motion correction matrix specific to the first volume ('0000').\n",
    "epi_moco = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-sitrep_run-1_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "\n",
    "# Timing the combination of transformations\n",
    "s0 = time.time()\n",
    "\n",
    "# Combine the motion correction, EPI-to-anatomical, and anatomical-to-standard space transformations\n",
    "combine_all_transforms(ref, warp_name, True, epi_2_moco=epi_moco, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "# 'combine_all_transforms' combines the transformations for the volume:\n",
    "# - The motion correction matrix (epi_moco).\n",
    "# - The warp from EPI to anatomical space (func_2_anat).\n",
    "# - The warp from anatomical to standard space (anat_2_mni_trans).\n",
    "# - The combined warp is saved as 'warp_name'.\n",
    "\n",
    "s1 = time.time()\n",
    "# Record the time after combining the transformations.\n",
    "\n",
    "# Define the output path for the transformed volume\n",
    "out_vol = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_std_vol' + split_nbr)\n",
    "# This is where the transformed first volume will be saved after applying the combined warp.\n",
    "\n",
    "# Apply the transformation using applywarp\n",
    "applywarp(target_epi, ref, out_vol, w=warp_name, rel=False)\n",
    "# 'applywarp' applies the combined warp to the target EPI volume ('target_epi'):\n",
    "# - The reference space is the MNI template ('ref').\n",
    "# - The output file will be saved as 'out_vol'.\n",
    "# - 'w=warp_name' specifies the combined warp file.\n",
    "# - 'rel=False' indicates that absolute coordinates are used for transformation.\n",
    "\n",
    "s2 = time.time()\n",
    "# Record the time after applying the transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider <b>grouping up the transformations to apply them</b>. As you will see, computing transformations can take time when they are non linear, whereas applying them is comparatively fast. We will investigate both grouping all transforms together or grouping all transforms which follow motion correction together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fsl.wrappers import applywarp\n",
    "\n",
    "# Set the reference volume to the MNI template for alignment\n",
    "ref = mni_template\n",
    "\n",
    "# Select the target volume: the first volume ('0000') of the EPI dataset\n",
    "target_epi = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_first-vol')\n",
    "split_nbr = '0000'  # The volume number we're working with\n",
    "\n",
    "# Define the name for the warp file that will be saved after combining transformations\n",
    "warp_name = op.join(preproc_root, 'sub-control01', 'func', 'sub-control1_split' + split_nbr + '_epi_2_std_warp')\n",
    "\n",
    "# Define paths to the necessary transformations\n",
    "func_2_anat = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_anat-space_warp.nii.gz')\n",
    "# Path for the warp from EPI to anatomical space, typically including fieldmap correction\n",
    "\n",
    "epi_moco = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "# Path for the motion correction matrix specific to the volume ('0000')\n",
    "\n",
    "# ----- START OF METHOD 1 -----\n",
    "# Record the start time for Method 1\n",
    "s0 = time.time()\n",
    "\n",
    "# Combine the transformations: motion correction, EPI-to-anatomical, and anatomical-to-standard space\n",
    "combine_all_transforms(ref, warp_name, True, epi_2_moco=epi_moco, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "\n",
    "# Record the time after combining the transformations\n",
    "s1 = time.time()\n",
    "\n",
    "# Define the output path for the transformed volume\n",
    "out_vol = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_std_vol' + split_nbr + '_v1')\n",
    "\n",
    "# Apply the combined warp to the EPI volume using the applywarp command\n",
    "# subprocess.run(['applywarp', '-i', target_epi, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs'])\n",
    "subprocess.run(['applywarp', '-i', target_epi, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs'])\n",
    "\n",
    "# Record the time after applying the transformation\n",
    "s2 = time.time()\n",
    "\n",
    "# ----- START OF METHOD 2 -----\n",
    "# Combine only the transformations post motion correction\n",
    "combine_all_transforms(ref, warp_name, True, epi_2_moco=None, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "\n",
    "# Record the time after combining the transformations for Method 2\n",
    "s3 = time.time()\n",
    "\n",
    "# Define the output path for the transformed volume in Method 2\n",
    "out_vol = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_std_vol' + split_nbr + '_v2')\n",
    "\n",
    "# Apply the transformation using the premat option for motion correction matrix\n",
    "subprocess.run(['applywarp', '-i', target_epi, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs', '--premat={}'.format(epi_moco)])\n",
    "\n",
    "# Record the time after applying the transformation\n",
    "s4 = time.time()\n",
    "\n",
    "# Print the runtime for Method 1: total time and breakdown (combination vs. application)\n",
    "print('Method 1 runtime:', s2 - s0, '({} for combination, {} to apply)'.format(s1 - s0, s2 - s1))\n",
    "\n",
    "# Print the runtime for Method 2: total time and breakdown (combination vs. application)\n",
    "print('Method 2 runtime:', s4 - s2, '({} for combination, {} to apply)'.format(s3 - s2, s4 - s3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two produced images are almost identical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(ref)\n",
    "fsleyesDisplay.load(op.join(preproc_root, 'sub-001', 'func', 'sub-control01_task-music_run-1_bold_std_vol' + split_nbr + '_v1'))\n",
    "fsleyesDisplay.load(op.join(preproc_root, 'sub-001', 'func', 'sub-control01_task-music_run-1_bold_std_vol' + split_nbr + '_v2'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does it matter? Well, just applying a back-of the envelope calculation, the first method takes 122s per volume, while the second method takes 87 seconds to combine **once** the transforms excluding motion correction, and 4 seconds per volume to apply the transforms including motion correction. If we plot the two with an increasing number of volumes, we can see why this quickly becomes relevant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying the transformation to the entire timeseries at last\n",
    "\n",
    "With all this in mind, let's now apply our transformation to all our volumes! The steps are:\n",
    "\n",
    "1. Split our EPI into all individual volumes (remember: applywarp only works on a single 3D image but our EPI is 4D).\n",
    "2. Combine all transformations from EPI after motion correction all the way to standard space **once**. \n",
    "3. Use applywarp for every volume, passing the motion correction transform of this volume and the EPI > standard space warp\n",
    "4. Combine back all volumes as a single 4D EPI in standard space\n",
    "\n",
    "Let's make sure you understand why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split our starting EPI volume across time \n",
    "\n",
    "# original_epi = op.join(bids_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold')\n",
    "split_target = original_epi\n",
    "split_name = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_split')\n",
    "\n",
    "subprocess.run(['fslsplit', split_target, split_name, '-t'])\n",
    "# This line uses the subprocess module to run the command 'fslsplit', which is an FSL tool for splitting 4D fMRI data.\n",
    "# 'fslsplit' splits the EPI volume ('split_target') into multiple 3D volumes based on time.\n",
    "# The resulting files are saved with the prefix defined in 'split_name'. The '-t' flag specifies that the split \n",
    "# should occur across the time dimension, meaning each resulting file will correspond to a single time point in the original 4D EPI volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dir_tree(bids_root,max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a lot of new volumes have appeared. These are the split, individual volumes of our EPI.\n",
    "\n",
    "Great, let's now combine the different transforms EXCEPT motion correction, with method 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Let's combine the different transforms EXCEPT motion correction!\n",
    "warp_name = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_epi_moco_2_std_warp')\n",
    "\n",
    "print(\"Starting to combine transforms...\")\n",
    "combine_all_transforms(ref, warp_name,  True, epi_2_moco=None, epi_2_anat_warp=func_2_anat, anat_2_standard_warp=anat_2_mni_trans)\n",
    "print(\"Done, moving on to application of transforms...\")\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "# Now apply transformation to all our volumes.\n",
    "# We will remember the volumes as well, to group them back afterwards.\n",
    "##########\n",
    "\n",
    "# Notice that we are sorting the volumes here! This is important, to make sure we don't get them in random order :)\n",
    "split_vols = sorted(glob.glob(op.join(preproc_root, 'sub-control01', 'func', '*_bold_split*')))\n",
    "\n",
    "\n",
    "# Define a function that wraps subprocess.run()\n",
    "def run_subprocess(split_vol, vol_nbr):\n",
    "    \"\"\"\n",
    "    SAFETY GOGGLES ON\n",
    "    This function launches applywarp in parallel to reach complete result quicker\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    split_vol: str\n",
    "        Path to the volume on which to apply the transformation\n",
    "    vol_nbr: str\n",
    "        Number of the volume in the timeserie. Useful to reorder volumes after the fact, since parallelisation does not honour order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out_vol: str\n",
    "        Path to the transformed volume\n",
    "    vol_nbr: str\n",
    "        Number of the volume in the timeserie. Useful to reorder volumes after the fact.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        split_nbr = split_vol.split('_')[-1].split('.')[0].split('split')[1]\n",
    "        epi_moco = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_moco.mat/', 'MAT_' + split_nbr)\n",
    "        out_vol= op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_std_vol' + split_nbr)\n",
    "        result = subprocess.run(['applywarp', '-i', split_vol, '-r', ref, '-o', out_vol, '-w', warp_name, '--abs', '--premat={}'.format(epi_moco)], check=True)\n",
    "        return out_vol, vol_nbr\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return f\"applywarp for volume '{split_vol}' failed with error: {e.stderr.decode('utf-8')}\"\n",
    "\n",
    "\n",
    "produced_vols = [None]*len(split_vols)\n",
    "# Initialize ThreadPoolExecutor and the progress bar\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Use tqdm to wrap the futures\n",
    "    with tqdm(total=len(split_vols)) as progress:\n",
    "        # Launch subprocesses in parallel\n",
    "        futures = {executor.submit(run_subprocess, vol,i): vol for i,vol in enumerate(split_vols)}\n",
    "\n",
    "        # Process completed tasks\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            out_vol, vol_nbr = future.result()  # Get the result of the subprocess\n",
    "            produced_vols[vol_nbr] = out_vol\n",
    "            # Update the progress bar for each completed task\n",
    "            progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we finally have our volumes!\n",
    "\n",
    "### Grouping the volumes together (Optional)\n",
    "\n",
    "Our volumes are now individually separated, but we started with a 4D volume.  In theory we should be able to group everything back. The issue is, our fMRI is now interpolated at 1 x 1 x 1 mm3 resolution, so it won't fit in RAM on this virtual machine, where a high performance computing cluster would do this without any issue.\n",
    "\n",
    "There are two answers to this issue. The first, used by fMRIprep, is to play it smart and actually never modify the fMRI's resolution.\n",
    "\n",
    "The second is to use to group back the files a memory map, that is to say a file on disk from which we only read the parts we need (VERY useful when you do not have enough RAM). Because writing to disk is very slow, we do it in a batch approach below.\n",
    "\n",
    "<div class=\"warning\" style='background-color:#C04000; color: #112A46; border-left: solid #C04000 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>âš ï¸  This part is TIME INTENSIVE and it is OPTIONAL âš ï¸ </b></p>\n",
    "<p style='text-indent: 10px;'>\n",
    "    This part will take TIME (about an hour, in fact)\n",
    "</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import progressbar\n",
    "\n",
    "first_vol = nib.load(produced_vols[0])\n",
    "v_shape = first_vol.get_fdata().shape\n",
    "\n",
    "preproc_root = '/home/jovyan/Data/dataset/ds004226/derivatives/preprocessed_data'\n",
    "filename = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_std.dat')\n",
    "large_array = np.memmap(filename, dtype=np.float64, mode='w+', shape=(v_shape[0],v_shape[1],v_shape[2], len(produced_vols)))\n",
    "\n",
    "batch_size = len(produced_vols)//4\n",
    "\n",
    "A = np.zeros((v_shape[0],v_shape[1],v_shape[2], batch_size))\n",
    "\n",
    "with progressbar.ProgressBar(max_value=len(produced_vols)) as bar:\n",
    "    for batch_i in range(4):\n",
    "        print('Starting for batch {}/4'.format(batch_i+1))\n",
    "        start_batch = batch_size * batch_i\n",
    "        end_batch = min(batch_size * (batch_i+1),len(produced_vols))\n",
    "        max_len = end_batch - start_batch + 1\n",
    "        for i in range(start_batch, end_batch):\n",
    "            vol = nib.load(produced_vols[i])\n",
    "            A[:,:,:,i-start_batch] = vol.get_fdata()\n",
    "            bar.update(i)\n",
    "        large_array[:,:,:, start_batch:end_batch] = A[:,:,:,:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, save to Nifti file using Nibabel\n",
    "\n",
    "# Step 1: Ensure all changes of the memmap are flushed to disk and then close it\n",
    "#large_array.flush()\n",
    "#del large_array\n",
    "print(\"Done flushing mmap\")\n",
    "large_array = np.memmap(filename, dtype=np.float64, mode='r', shape=(v_shape[0],v_shape[1],v_shape[2], len(produced_vols)))\n",
    "\n",
    "# Step 2: Modify the header to indicate that we have 4D data, and specify its TR.\n",
    "header = first_vol.header.copy()  # Copy the header of the first volume (to get right resolution, affine, Q-form etc)\n",
    "header['dim'][0] = 4  # Specifies that this is a 4D dataset\n",
    "header['dim'][1:5] = large_array.shape  # Update dimensions (x, y, z, t)\n",
    "header['pixdim'][4] = 1.5  # Set the TR in the 4th dimension. You can see the TR of the data by looking at your original EPI series with fslhd, remember ;)\n",
    "print(\"Done with header\")\n",
    "\n",
    "# Step 3: Create the Nifti1 image and save it to disk\n",
    "mni_epi = op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_mni.nii.gz')\n",
    "img = nib.Nifti1Image(large_array, first_vol.affine, first_vol.header)\n",
    "print(\"Done creating the image\")\n",
    "img.to_filename(mni_epi)\n",
    "print(\"Done writing it to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can move on to removing all the temporary files we used, to end up with only one clean Nifti file :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('rm -rf {}'.format(op.join(preproc_root, 'sub-control01', 'func', 'sub-control01_task-music_run-1_bold_split*.nii.gz')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(ref)\n",
    "fsleyesDisplay.load(produced_vols[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we?\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Field unwarping</td><td style='text-align:justify;'>Correction distortions induced by inhomogeneities of the b0 field through maps acquired specifically to measure this field called fieldmaps.</td><td style='text-align:justify;'>FUGUE (but also FLIRT - see below)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Slice-timing correction</td><td style='text-align:justify;'>Accounting for the difference in acquisition between the slices that make up a volume to interpolate back voxels to a fixed time reference.</td><td style='text-align:justify;'>slicetimer</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice correction \n",
    "\n",
    "#### Application to real data\n",
    "\n",
    "We have shown you the basic principle, but the application to real data requires some specific informations.\n",
    "You need the following ingredients:\n",
    "- When was each slice acquired in the sequence: **(Slice timing)**\n",
    "- Along which axis were the slices acquired: **Phase direction**\n",
    "- How much time we take to acquire all slices: **TR**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_json_from_file(op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.json'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is actually a dictionary. We can thus extract the slice timing as an array directly from it. For example, to extract TaskName, we would use:\n",
    "```python\n",
    "data['TaskName']\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_timing = data['SliceTiming'] # Replace with the appropriate key (have a look above!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we might want to know where our slices are, ie along which axis, right? Typically it is along the z-direction, but we're better off if we check! Using FSLeyes, determine how many slices each axis has **for the functional data of interest**. You should thus open the relevant functional file in FSLeyes to answer this question.\n",
    "\n",
    "\n",
    "<div class=\"warning\" style='background-color:#90EE90; color: #112A46; border-left: solid #805AD5 4px; border-radius: 4px; padding:0.7em;'>\n",
    "<span>\n",
    "<p style='margin-top:1em; text-align:center'><b>Using FSL command line</b></p>\n",
    "<p style='text-indent: 10px;'>To figure out the dimensions of an MRI image, a faster option - if you have FSL installed directly - is to run the command line command:\n",
    "    <blockquote>fslhd [your_volume]</blockquote>\n",
    "This will give you all informations contained within the header of the NIfti file. For example, running the command for our volume will easily allow us to access the slice informations:\n",
    "    <img src=\"imgs/fslhd_capture.png\"></p>\n",
    "</span>\n",
    "</div>\n",
    "Let's compare now with the amount of slices we have in our acquisition. We can consider simply the number of timings for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(slice_timing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so we know which axis we want, we know the slice timings, but we still need to know the TR. This information is also in the JSON sidecar! Extract it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = data['RepetitionTime'] # Extract the TR from the sidecar's appropriate field\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To now perform the correction, we need to apply FSL's slicetimer command. For this, we need to save the timings first to their own separate file! Instead of giving the slice timings, we will provide instead the slice **order** (ie which slice was done in which order) and let FSL figure out how to best correct based on this information.\n",
    "\n",
    "Let's do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_order = np.argsort(slice_timing) + 1\n",
    "\n",
    "# Write to a file the corresponding sorted timings :)\n",
    "timing_path = op.join(preproc_root,  'sub-001', 'func', 'sub-001_task-sitrep_run-01_slice-timings.txt')\n",
    "file = open(timing_path, mode='w')\n",
    "for t in slice_order:\n",
    "    file.write(str(t) + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can call slicetimer from a terminal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_realign = op.join(bids_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold')\n",
    "output_target = op.join(preproc_root, 'sub-001', 'func', 'sub-001_task-sitrep_run-01_bold_slice-corr')\n",
    "\n",
    "subprocess.run(['slicetimer', '-i', file_to_realign, '-o', output_target, '-r', str(tr), '-d', str(3), '--ocustom={}'.format(timing_path)])\n",
    "#cmd = 'slicetimer -i ' + file_to_realign + ' -o ' + output_target + ' -r ' + str(tr) + ' -d 3 --ocustom=' + timing_path\n",
    "#os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsleyesDisplay.resetOverlays()\n",
    "fsleyesDisplay.load(file_to_realign)\n",
    "fsleyesDisplay.load(output_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI and fMRI preprocessing: conclusion\n",
    "\n",
    "You have reached the end the preprocessing for MRI and fMRI. \n",
    "\n",
    "As you can see, there are many fairly involved steps that need to be conducted.\n",
    "\n",
    "To normalize choices and results, software solutions exist, such as the excellent <a href=\"https://fmriprep.org/en/stable/\">fmriprep</a>, to conduct automatically all steps for you while providing you with quality checks to verify that all went well, as no algorithm can replace your expert eye to inspect potential artefacts and remove them.\n",
    "\n",
    "These tools are nonetheless precious to help different groups follow same systematic choices of preprocessing, both in parameters and order of application for each method, leading to more reproducible science in the long run. (But they require a big RAM and take hours to run, which is why we've avoided them for the purpose of this tutorial :) )\n",
    "\n",
    "Let's wrap up what you have learnt.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr><th style='text-align:justify;'>Data type</th><th style='text-align:justify;'>Step name </th><th style='text-align:justify;'>Details of the step</th><th style='text-align:justify;'>FSL tool </th></tr>\n",
    "    <tr><th>Anatomical</th><td></td><td></td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Skull stripping</td><td style='text-align:justify;'>Removing skull and surrounding tissues to keep only the brain</td><td style='text-align:justify;'>BET</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Segmentation</td><td style='text-align:justify;'>Segmenting brain tissues based on their contrasts</td><td style='text-align:justify;'>FAST</td></tr>\n",
    "    <tr><td></td><td style='text-align:justify;'>Normalization</td><td style='text-align:justify;'>Mapping participant's brain to a reference brain, making its orientation and scale match so that comparison across participants become feasible.</td><td style='text-align:justify;'>FLIRT</td></tr>\n",
    "    <tr><th>Functional</th><th></th><th></th></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>First few volumes removal</td><td style='text-align:justify;'>Removing volumes for which the B0 field is still not stable and that could contaminate all our data if left unchecked.</td><td style='text-align:justify;'>fslroi</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Motion correction</td><td style='text-align:justify;'>Realignment of fMRI volumes to a common reference - typically one volume or the average of the volumes - to correct for inter-volume motion. The extracted motion parameters can be used for subsequent analysis (see GLM in one week!)</td><td style='text-align:justify;'>MCFLIRT (which is one suboption of FLIRT in fact)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Fieldmap preparation</td><td style='text-align:justify;'>Field maps can be used to create a distortion field to correct...Distortions. </td><td style='text-align:justify;'>topup, FUGUE</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Coregistration</td><td style='text-align:justify;'>Realignment of fMRI volumes to anatomical space - the subject's own MRI is typically used. We can include susceptibility-distortion correction with fieldmaps. We compute this transformation only for the volume we used as reference in MCFLIRT. Then, we apply it to all other volumes in the EPI.</td><td style='text-align:justify;'>epi_reg (FLIRT with boundary-based registration)</td></tr>\n",
    "    <tr><td></td><td style='text-align:left;'>Normalization</td><td style='text-align:justify;'>Putting the EPI in a template space, such as MNI. This is done by applying the transformation of anatomical space to MNI space, which was computed in the anatomical preprocessing. Note that we typically like to combine this transform with coregistration to do everything in one go.</td><td style='text-align:justify;'>applywarp to apply combined warps, otherwise (if going transform by transform), FLIRT with applyxfm option</td></tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "If you have any more questions, both on these tools and on preprocessing, do not hesitate!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
